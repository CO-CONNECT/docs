{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the documentation of the CO-CONNECT project! CO-CONNECT Bringing together COVID-19 data from across the UK What we do in a nutshell.. reasearcher . data = coconnect . find ( 'patient_data' , fields = [ 'birth_date' , 'ethnicity' , 'existing_conditions' ] anonymous = True )","title":"Welcome"},{"location":"AzureFunctions/RunningLocally/","text":"Introduction \u00b6 Running Azure Functions locally is a little involved; this guide has been written to get you up and running with Azure CLI and Functions. Note that this guide assumes you're using VSCode. All of the functionality described here can be replicated outside of VSCode (in Azure CLI) but instructions for doing so are not detailed in this guide. Note that this guide is not intended to demonstrate how to create an Azure Function from scratch. Its purpose is to show you how to run pre-coded Functions. To follow this guide you need to install Azure CLI . Azure Functions Basics \u00b6 Conceptually, queue-based Azure Functions are simple to follow. A message is posted to a message queue . A Function then executes on messages within a message queue. In Co-Connect, a message is posted to a message queue as JSON but other formats are possible (e.g. XML). Here's an example message destined for the scanreports queue: { \"scan_report_id\": 105, \"blob_name\": \"GOSH_CoStar_WhiteRabbit_MetaData_V24_T29kygl.xlsx\" } And here is an example message destined for the nlpqueue queue: { \"documents\": [{ \"language\": \"en\", \"id\": \"17730_field\", \"text\": \"The patient has a fever\" }] } An Azure function is made up of three key files. Two are kept in the function's directory (which itself lives within Co-Connect's root directory) and define what and how the function should run: init.py - This contains the function's logic. The method main() should be present for the function to execute. function.json - This tells Azure what type of function you're developing and also what queue from which to pick messages up from. Another file is kept outside of the function's directory and should not be commited to Git: local.settings.json - Contains environment variables for your function. It must be present to run Azure functions locally. Contact a member of the development team to get all keys and connection strings for this file. For reference, a local.settings.json file looks like this: { \"IsEncrypted\": false, \"Values\": { \"AzureWebJobsStorage\": \"REDACTED, \"FUNCTIONS_WORKER_RUNTIME\": \"python\", \"STORAGE_CONN_STRING\": \"REDACTED\", \"NLP_API_KEY\": \"REDACTED\", \"APP_URL\": \"REDACTED\", \"AZ_FUNCTION_KEY\": \"REDACTED\", \"SCAN_REPORT_QUEUE_NAME\": \"scanreports-local\", \"NLP_QUEUE_NAME\": \"nlpqueue-local\" } } You only need to have 1 local.settings.json file for all functions within the Co-Connect project. Note, however, that it should be updated with extra queue names if/when the project requires them. Azure Functions in Co-Connect \u00b6 As of July 2021, the Co-Connect project has two different Azure Functions: one for processing scan reports (called ProcessQueue which posts to the message queue called scanreports ), another to run the NLP service (called NLPQueue which posts to the message queue called nlpqueue ). The code for these functions lives in the directories ProcessQueue and NLPQueue, respectively. Azure Functions Queues \u00b6 For each process (NLP and Scan Reports), we have two queues each - a 'local' queue and a 'live' queue (for a total of 4 unique queues.) As environment variables, they are encoded as: Local queues \u00b6 NLP_QUEUE_NAME=nlpqueue-local SCAN_REPORT_QUEUE_NAME=scanreports-local Deployed queues \u00b6 NLP_QUEUE_NAME=nlpqueue SCAN_REPORT_QUEUE_NAME=scanreports The purpose of the local queues is that, when developing locally, messages are sent only to a 'local' queue. This stops 'development' messages from posting to the 'live' message queue. These environment variables which control where messages are sent must be maintained in the following locations: local.settings.json - A local file to hold Azure Function environment variables. Should point to 'local' (e.g. NLP_QUEUE_NAME=nlpqueue-local ). To reiterate, this file must not be tracked on Github, otherwise secrets will be exposed! Within the Azure Function App. Env Vars need to be set within the Azure Portal to the 'live' vars. Speak to Sam Cox about adding vars on the Azure Portal. Within the CCOM webapp, the .env file must include variables for 'local' on your local machine and set to 'live' variables on App Service. Running Azure Functions Locally \u00b6 Because Azure Functions are cloud-based, it's somewhat of a misnomer to talk of running an Azure Function completely 'locally'. In reality, you're still posting to a message queue in the cloud, even when developing locally. However, Azure CLI--when running in debug mode (more on this later)--will 'hijack' the messages in the message queue (depending on how you've set the environment vairables in local.settings.json and .env ) and allow you to process them with the code you're developing locally. To start debugging locally in VSCode you must first ensure that CCOM is up and running locally ( see here for building and running the Co-Connect Docker image ). Once your local server is running, you can start Azure Function's debugging mode by clicking: Run (top toolbar) -> Start Debugging After a few seconds, you'll see something like the following printed to the debugging terminal that's started when you run debugging: > Executing task: . .venv/bin/activate && func host start < Found Python version 3.9.5 (python3). Azure Functions Core Tools Core Tools Version: 3.0.3477 Commit hash: 5fbb9a76fc00e4168f2cc90d6ff0afe5373afc6d (64-bit) Function Runtime Version: 3.0.15584.0 [2021-07-02T09:34:28.929Z] Cannot create directory for shared memory usage: /dev/shm/AzureFunctions [2021-07-02T09:34:28.929Z] System.IO.FileSystem: Access to the path '/dev/shm/AzureFunctions' is denied. Operation not permitted. Functions: NLPQueue: queueTrigger ProcessQueue: queueTrigger For detailed output, run func with --verbose flag. [2021-07-02T09:34:32.711Z] Traceback (most recent call last): [2021-07-02T09:34:32.711Z] File \"/usr/local/Cellar/azure-functions-core-tools@3/3.0.3477/workers/python/3.9/OSX/X64/azure_functions_worker/bindings/shared_memory_data_transfer/file_accessor_unix.py\", line 127, in _get_valid_mem_map_dirs [2021-07-02T09:34:32.711Z] os.makedirs(dir_path) [2021-07-02T09:34:32.711Z] File \"/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/os.py\", line 215, in makedirs [2021-07-02T09:34:32.711Z] makedirs(head, exist_ok=exist_ok) [2021-07-02T09:34:32.711Z] File \"/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/os.py\", line 225, in makedirs [2021-07-02T09:34:32.711Z] mkdir(name, mode) [2021-07-02T09:34:32.711Z] PermissionError: [Errno 1] Operation not permitted: '/dev/shm' [2021-07-02T09:34:32.812Z] Worker process started and initialized. [2021-07-02T09:34:36.731Z] Host lock lease acquired by instance ID '000000000000000000000000DC4198B8'. You'll likely see a few errors/warnings about directories and permissions. At the time of writing this guide, it's safe to ignore these! Note that two functions are listed as you start debugging mode: NLPQueue and ProcessQueue . These are the names of the directories which hold the function's code, not the name of the message queue which holds the function's messages. The text after the function name (queueTrigger) specifies the type of function it is (defined in function.json .) So far in Co-Connect, only queueTriggers are used. For more information on Function types, click here . With the final console output saying 'Host lock lease acquired', you're ready to run your first Azure Functions job! Running an NLP task \u00b6 The remainder of this guide will show what to expect to see in the debugging console if you run NLP at the field level. Navigate to a field within a scan report. Ensure the field you've chosen has a meaningful description (e.g. \"Patient has a cold\"). If it doesn't have one, click 'Edit Field' and manually put one in. Ensure that pass_from_source is set to True within the field's settings. This will cause CCOM to process the field only. Click 'Run NLP' After a short wait a green message banner will appear to say NLP is now running. Hop back to VSCode. After a few seconds, you should see the debugging console updating to say that it has received the message from the message queue: Executing 'Functions.NLPQueue' (Reason='New queue message detected on 'nlpqueue-local'.', Id=ea27d6be-eb0b-490c-9cab-82cf2a119355) Trigger Details: MessageId: f00a438f-9bad-4110-bb8b-5278461d7bb3, DequeueCount: 1, InsertionTime: 2021-07-02T10:57:26.000+00:00 If you have configured .env and local.settings.json correctly, you will see that the job was posted to the queue nlpqueue-local . After some time (around 15-30 seconds), you should notice further updates to the debugging console. Due to various print() statements in the init.py file (remember, this holds the function's logic), the console will start logging what's occurring as the function processes the message in the message queue. First, the console shows you what message was sent to the queue: MESSAGE >>> {'id': 'f00a438f-9bad-4110-bb8b-5278461d7bb3', 'body': '{\"documents\": [{\"language\": \"en\", \"id\": \"17569_field\", \"text\": \"patient has a cold\"}]}'} After job submission, the NLPQueue function works through the stages detailed in NLP Processing . Having received concept codes from the NLP API, the function looks up a standard and valid conceptID using the CCOM API. Sometimes, a concept code will be returned which doesn't have a standard and valid conceptID. In such instances you will see the following in the debugging console: Concept Code C34500 not found! However, if a standard and valid conceptID is found, the Azure function then saves the data to the ScanReportConcept model. This is shown to you in the debugging console with the PAYLOAD print statement like so: SAVING TO FIELD LEVEL... PAYLOAD >>> {'nlp_entity': 'cold', 'nlp_entity_type': 'SymptomOrSign', 'nlp_confidence': 0.75, 'nlp_vocabulary': 'ICD10CM', 'nlp_concept_code': 'J00', 'concept': 260427, 'object_id': '17569', 'content_type': 15} SAVING TO FIELD LEVEL... PAYLOAD >>> {'nlp_entity': 'cold', 'nlp_entity_type': 'SymptomOrSign', 'nlp_confidence': 0.75, 'nlp_vocabulary': 'ICD9CM', 'nlp_concept_code': '460', 'concept': 260427, 'object_id': '17569', 'content_type': 15} SAVING TO FIELD LEVEL... PAYLOAD >>> {'nlp_entity': 'cold', 'nlp_entity_type': 'SymptomOrSign', 'nlp_confidence': 0.75, 'nlp_vocabulary': 'SNOMEDCT_US', 'nlp_concept_code': '82272006', 'concept': 260427, 'object_id': '17569', 'content_type': 15} Here, three conceptIDs were found for three different vocabularies (ICD10CM, ICD9CM and SNOWMEDCT_US). Therefore, three records are saved to ScanReportConcepts (hence the three SAVING TO FIELD LEVEL... statements) After successful execution, the debug console will finally tell you that the job has finished: Executed 'Functions.NLPQueue' (Succeeded, Id=ea27d6be-eb0b-490c-9cab-82cf2a119355, Duration=30234ms) Here is a full interrupted trace of the above process: Executing 'Functions.NLPQueue' (Reason='New queue message detected on 'nlpqueue-local'.', Id=ea27d6be-eb0b-490c-9cab-82cf2a119355) Trigger Details: MessageId: f00a438f-9bad-4110-bb8b-5278461d7bb3, DequeueCount: 1, InsertionTime: 2021-07-02T10:57:26.000+00:00 MESSAGE >>> {'id': 'f00a438f-9bad-4110-bb8b-5278461d7bb3', 'body': '{\"documents\": [{\"language\": \"en\", \"id\": \"17569_field\", \"text\": \"patient has a cold\"}]}'} Concept Code C34500 not found! SAVING TO FIELD LEVEL... PAYLOAD >>> {'nlp_entity': 'cold', 'nlp_entity_type': 'SymptomOrSign', 'nlp_confidence': 0.75, 'nlp_vocabulary': 'ICD10CM', 'nlp_concept_code': 'J00', 'concept': 260427, 'object_id': '17569', 'content_type': 15} SAVING TO FIELD LEVEL... PAYLOAD >>> {'nlp_entity': 'cold', 'nlp_entity_type': 'SymptomOrSign', 'nlp_confidence': 0.75, 'nlp_vocabulary': 'ICD9CM', 'nlp_concept_code': '460', 'concept': 260427, 'object_id': '17569', 'content_type': 15} SAVING TO FIELD LEVEL... PAYLOAD >>> {'nlp_entity': 'cold', 'nlp_entity_type': 'SymptomOrSign', 'nlp_confidence': 0.75, 'nlp_vocabulary': 'SNOMEDCT_US', 'nlp_concept_code': '82272006', 'concept': 260427, 'object_id': '17569', 'content_type': 15} Executed 'Functions.NLPQueue' (Succeeded, Id=ea27d6be-eb0b-490c-9cab-82cf2a119355, Duration=30234ms) Stopping Debugging \u00b6 Debugging can be stopped by clicking: Run (top menu) -> Stop Debugging","title":"LocalFunctions"},{"location":"AzureFunctions/RunningLocally/#introduction","text":"Running Azure Functions locally is a little involved; this guide has been written to get you up and running with Azure CLI and Functions. Note that this guide assumes you're using VSCode. All of the functionality described here can be replicated outside of VSCode (in Azure CLI) but instructions for doing so are not detailed in this guide. Note that this guide is not intended to demonstrate how to create an Azure Function from scratch. Its purpose is to show you how to run pre-coded Functions. To follow this guide you need to install Azure CLI .","title":"Introduction"},{"location":"AzureFunctions/RunningLocally/#azure-functions-basics","text":"Conceptually, queue-based Azure Functions are simple to follow. A message is posted to a message queue . A Function then executes on messages within a message queue. In Co-Connect, a message is posted to a message queue as JSON but other formats are possible (e.g. XML). Here's an example message destined for the scanreports queue: { \"scan_report_id\": 105, \"blob_name\": \"GOSH_CoStar_WhiteRabbit_MetaData_V24_T29kygl.xlsx\" } And here is an example message destined for the nlpqueue queue: { \"documents\": [{ \"language\": \"en\", \"id\": \"17730_field\", \"text\": \"The patient has a fever\" }] } An Azure function is made up of three key files. Two are kept in the function's directory (which itself lives within Co-Connect's root directory) and define what and how the function should run: init.py - This contains the function's logic. The method main() should be present for the function to execute. function.json - This tells Azure what type of function you're developing and also what queue from which to pick messages up from. Another file is kept outside of the function's directory and should not be commited to Git: local.settings.json - Contains environment variables for your function. It must be present to run Azure functions locally. Contact a member of the development team to get all keys and connection strings for this file. For reference, a local.settings.json file looks like this: { \"IsEncrypted\": false, \"Values\": { \"AzureWebJobsStorage\": \"REDACTED, \"FUNCTIONS_WORKER_RUNTIME\": \"python\", \"STORAGE_CONN_STRING\": \"REDACTED\", \"NLP_API_KEY\": \"REDACTED\", \"APP_URL\": \"REDACTED\", \"AZ_FUNCTION_KEY\": \"REDACTED\", \"SCAN_REPORT_QUEUE_NAME\": \"scanreports-local\", \"NLP_QUEUE_NAME\": \"nlpqueue-local\" } } You only need to have 1 local.settings.json file for all functions within the Co-Connect project. Note, however, that it should be updated with extra queue names if/when the project requires them.","title":"Azure Functions Basics"},{"location":"AzureFunctions/RunningLocally/#azure-functions-in-co-connect","text":"As of July 2021, the Co-Connect project has two different Azure Functions: one for processing scan reports (called ProcessQueue which posts to the message queue called scanreports ), another to run the NLP service (called NLPQueue which posts to the message queue called nlpqueue ). The code for these functions lives in the directories ProcessQueue and NLPQueue, respectively.","title":"Azure Functions in Co-Connect"},{"location":"AzureFunctions/RunningLocally/#azure-functions-queues","text":"For each process (NLP and Scan Reports), we have two queues each - a 'local' queue and a 'live' queue (for a total of 4 unique queues.) As environment variables, they are encoded as:","title":"Azure Functions Queues"},{"location":"AzureFunctions/RunningLocally/#running-azure-functions-locally","text":"Because Azure Functions are cloud-based, it's somewhat of a misnomer to talk of running an Azure Function completely 'locally'. In reality, you're still posting to a message queue in the cloud, even when developing locally. However, Azure CLI--when running in debug mode (more on this later)--will 'hijack' the messages in the message queue (depending on how you've set the environment vairables in local.settings.json and .env ) and allow you to process them with the code you're developing locally. To start debugging locally in VSCode you must first ensure that CCOM is up and running locally ( see here for building and running the Co-Connect Docker image ). Once your local server is running, you can start Azure Function's debugging mode by clicking: Run (top toolbar) -> Start Debugging After a few seconds, you'll see something like the following printed to the debugging terminal that's started when you run debugging: > Executing task: . .venv/bin/activate && func host start < Found Python version 3.9.5 (python3). Azure Functions Core Tools Core Tools Version: 3.0.3477 Commit hash: 5fbb9a76fc00e4168f2cc90d6ff0afe5373afc6d (64-bit) Function Runtime Version: 3.0.15584.0 [2021-07-02T09:34:28.929Z] Cannot create directory for shared memory usage: /dev/shm/AzureFunctions [2021-07-02T09:34:28.929Z] System.IO.FileSystem: Access to the path '/dev/shm/AzureFunctions' is denied. Operation not permitted. Functions: NLPQueue: queueTrigger ProcessQueue: queueTrigger For detailed output, run func with --verbose flag. [2021-07-02T09:34:32.711Z] Traceback (most recent call last): [2021-07-02T09:34:32.711Z] File \"/usr/local/Cellar/azure-functions-core-tools@3/3.0.3477/workers/python/3.9/OSX/X64/azure_functions_worker/bindings/shared_memory_data_transfer/file_accessor_unix.py\", line 127, in _get_valid_mem_map_dirs [2021-07-02T09:34:32.711Z] os.makedirs(dir_path) [2021-07-02T09:34:32.711Z] File \"/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/os.py\", line 215, in makedirs [2021-07-02T09:34:32.711Z] makedirs(head, exist_ok=exist_ok) [2021-07-02T09:34:32.711Z] File \"/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/os.py\", line 225, in makedirs [2021-07-02T09:34:32.711Z] mkdir(name, mode) [2021-07-02T09:34:32.711Z] PermissionError: [Errno 1] Operation not permitted: '/dev/shm' [2021-07-02T09:34:32.812Z] Worker process started and initialized. [2021-07-02T09:34:36.731Z] Host lock lease acquired by instance ID '000000000000000000000000DC4198B8'. You'll likely see a few errors/warnings about directories and permissions. At the time of writing this guide, it's safe to ignore these! Note that two functions are listed as you start debugging mode: NLPQueue and ProcessQueue . These are the names of the directories which hold the function's code, not the name of the message queue which holds the function's messages. The text after the function name (queueTrigger) specifies the type of function it is (defined in function.json .) So far in Co-Connect, only queueTriggers are used. For more information on Function types, click here . With the final console output saying 'Host lock lease acquired', you're ready to run your first Azure Functions job!","title":"Running Azure Functions Locally"},{"location":"AzureFunctions/RunningLocally/#running-an-nlp-task","text":"The remainder of this guide will show what to expect to see in the debugging console if you run NLP at the field level. Navigate to a field within a scan report. Ensure the field you've chosen has a meaningful description (e.g. \"Patient has a cold\"). If it doesn't have one, click 'Edit Field' and manually put one in. Ensure that pass_from_source is set to True within the field's settings. This will cause CCOM to process the field only. Click 'Run NLP' After a short wait a green message banner will appear to say NLP is now running. Hop back to VSCode. After a few seconds, you should see the debugging console updating to say that it has received the message from the message queue: Executing 'Functions.NLPQueue' (Reason='New queue message detected on 'nlpqueue-local'.', Id=ea27d6be-eb0b-490c-9cab-82cf2a119355) Trigger Details: MessageId: f00a438f-9bad-4110-bb8b-5278461d7bb3, DequeueCount: 1, InsertionTime: 2021-07-02T10:57:26.000+00:00 If you have configured .env and local.settings.json correctly, you will see that the job was posted to the queue nlpqueue-local . After some time (around 15-30 seconds), you should notice further updates to the debugging console. Due to various print() statements in the init.py file (remember, this holds the function's logic), the console will start logging what's occurring as the function processes the message in the message queue. First, the console shows you what message was sent to the queue: MESSAGE >>> {'id': 'f00a438f-9bad-4110-bb8b-5278461d7bb3', 'body': '{\"documents\": [{\"language\": \"en\", \"id\": \"17569_field\", \"text\": \"patient has a cold\"}]}'} After job submission, the NLPQueue function works through the stages detailed in NLP Processing . Having received concept codes from the NLP API, the function looks up a standard and valid conceptID using the CCOM API. Sometimes, a concept code will be returned which doesn't have a standard and valid conceptID. In such instances you will see the following in the debugging console: Concept Code C34500 not found! However, if a standard and valid conceptID is found, the Azure function then saves the data to the ScanReportConcept model. This is shown to you in the debugging console with the PAYLOAD print statement like so: SAVING TO FIELD LEVEL... PAYLOAD >>> {'nlp_entity': 'cold', 'nlp_entity_type': 'SymptomOrSign', 'nlp_confidence': 0.75, 'nlp_vocabulary': 'ICD10CM', 'nlp_concept_code': 'J00', 'concept': 260427, 'object_id': '17569', 'content_type': 15} SAVING TO FIELD LEVEL... PAYLOAD >>> {'nlp_entity': 'cold', 'nlp_entity_type': 'SymptomOrSign', 'nlp_confidence': 0.75, 'nlp_vocabulary': 'ICD9CM', 'nlp_concept_code': '460', 'concept': 260427, 'object_id': '17569', 'content_type': 15} SAVING TO FIELD LEVEL... PAYLOAD >>> {'nlp_entity': 'cold', 'nlp_entity_type': 'SymptomOrSign', 'nlp_confidence': 0.75, 'nlp_vocabulary': 'SNOMEDCT_US', 'nlp_concept_code': '82272006', 'concept': 260427, 'object_id': '17569', 'content_type': 15} Here, three conceptIDs were found for three different vocabularies (ICD10CM, ICD9CM and SNOWMEDCT_US). Therefore, three records are saved to ScanReportConcepts (hence the three SAVING TO FIELD LEVEL... statements) After successful execution, the debug console will finally tell you that the job has finished: Executed 'Functions.NLPQueue' (Succeeded, Id=ea27d6be-eb0b-490c-9cab-82cf2a119355, Duration=30234ms) Here is a full interrupted trace of the above process: Executing 'Functions.NLPQueue' (Reason='New queue message detected on 'nlpqueue-local'.', Id=ea27d6be-eb0b-490c-9cab-82cf2a119355) Trigger Details: MessageId: f00a438f-9bad-4110-bb8b-5278461d7bb3, DequeueCount: 1, InsertionTime: 2021-07-02T10:57:26.000+00:00 MESSAGE >>> {'id': 'f00a438f-9bad-4110-bb8b-5278461d7bb3', 'body': '{\"documents\": [{\"language\": \"en\", \"id\": \"17569_field\", \"text\": \"patient has a cold\"}]}'} Concept Code C34500 not found! SAVING TO FIELD LEVEL... PAYLOAD >>> {'nlp_entity': 'cold', 'nlp_entity_type': 'SymptomOrSign', 'nlp_confidence': 0.75, 'nlp_vocabulary': 'ICD10CM', 'nlp_concept_code': 'J00', 'concept': 260427, 'object_id': '17569', 'content_type': 15} SAVING TO FIELD LEVEL... PAYLOAD >>> {'nlp_entity': 'cold', 'nlp_entity_type': 'SymptomOrSign', 'nlp_confidence': 0.75, 'nlp_vocabulary': 'ICD9CM', 'nlp_concept_code': '460', 'concept': 260427, 'object_id': '17569', 'content_type': 15} SAVING TO FIELD LEVEL... PAYLOAD >>> {'nlp_entity': 'cold', 'nlp_entity_type': 'SymptomOrSign', 'nlp_confidence': 0.75, 'nlp_vocabulary': 'SNOMEDCT_US', 'nlp_concept_code': '82272006', 'concept': 260427, 'object_id': '17569', 'content_type': 15} Executed 'Functions.NLPQueue' (Succeeded, Id=ea27d6be-eb0b-490c-9cab-82cf2a119355, Duration=30234ms)","title":"Running an NLP task"},{"location":"AzureFunctions/RunningLocally/#stopping-debugging","text":"Debugging can be stopped by clicking: Run (top menu) -> Stop Debugging","title":"Stopping Debugging"},{"location":"AzureFunctions/processQueue/","text":"Introduction \u00b6 ProcessQueue refers to the Azure QueueTrigger function which executes when a new message appears in the Scan Report queue. A new message is added when a Scan Report file is uploaded from the user. Triggering the function \u00b6 When a White Rabbit Scan Report file is uploaded on the site, it is saved into Azure Blob Storage and a message is send to the storage Queue. This message includes the scan_report_id from the ScanReport model and blob_name which is the name of the uploaded file. The queue message needs to be encoded before being sent to the queue for the ProcessQueue to access it correctly. azure_dict = { \"scan_report_id\" : scan_report . id , \"blob_name\" : str ( scan_report . file ) } queue_message = json . dumps ( azure_dict ) message_bytes = queue_message . encode ( 'ascii' ) base64_bytes = base64 . b64encode ( message_bytes ) base64_message = base64_bytes . decode ( 'ascii' ) queue = QueueClient . from_connection_string ( conn_str =< connection string > , queue_name =< queue name > ) queue . send_message ( base64_message ) Figure 1 Triggering the Queue when uploading a Scan Report Accessing the File \u00b6 As soon as the ProcessQueue is triggered it uses the blob_name from the message body to download the file from Azure Blob Storage into a BytesIO() object. Processing \u00b6 The next step is to step through the Excel workbook and create model entries for each ScanReportTable , ScanReportField and ScanReportValue present in the file. Scan Report Table \u00b6 The Field Overview sheet in a White Rabbit Scan Report has a 'Table' column which seperates different table names using a blank row. ProcessQueue main method iterates through the rows in the 'Table' column and appends each unique table name to the list table_names . For each table in table_names a new ScanReportTable entry is generated. The table entries are linked to the ScanReport model using the scan_report_id from the queue message body. The table entries are then appended to a JSON array json_data which forms the input send in a POST request to the API . The API automatically generates the id for each table that was created. These ids are extracted from the POST request response and saved in a list ( table_ids ) Scan Report Field \u00b6 Field Overview contains all the information on the fields including the name, description and other useful columns. ProcessQueue iterates through the rows in the sheet and generates a new ScanReportField entry. The field entries are linked to the relevant ScanReportTable using the previously generated table_ids . As soon as it detects an empty line (different table) it links to the next table_id from the list. Similarly as in Scan Report Table the field entries are appended to a JSON array json_data , and form the input to a POST request made to the API . From the response it saves the field_ids and field_names and stores them into a dictionary as key value pairs e.g \"Field Name\": Field ID. Scan Report Value \u00b6 Scan report values are stored in sheets named after their corresponding table. For every sheet after the initial two 'Field Overview' and 'Table Overview'(not currently used), process_scan_report_sheet_table() is called. The function extracts the data in the following format: Input ID Frequency Date Frequency 1 20 02/12/2020 5 2 3 12/11/2020 37 Output (ID, 1, 20) (Date, 02/12/2020, 5) (ID, 2, 3) (Date, 12/11/2020, 37) A ScanReportValue entry is generated by iterating through the output of process_scan_report_sheet_table() . The value entries are linked to the ScanReportField model using the function output and the dictionary with all the field names and ids. Same as with the other two models the value entries are appended to a JSON array json_data , and form the input to a POST request made to the API .","title":"ProcessQueue"},{"location":"AzureFunctions/processQueue/#introduction","text":"ProcessQueue refers to the Azure QueueTrigger function which executes when a new message appears in the Scan Report queue. A new message is added when a Scan Report file is uploaded from the user.","title":"Introduction"},{"location":"AzureFunctions/processQueue/#triggering-the-function","text":"When a White Rabbit Scan Report file is uploaded on the site, it is saved into Azure Blob Storage and a message is send to the storage Queue. This message includes the scan_report_id from the ScanReport model and blob_name which is the name of the uploaded file. The queue message needs to be encoded before being sent to the queue for the ProcessQueue to access it correctly. azure_dict = { \"scan_report_id\" : scan_report . id , \"blob_name\" : str ( scan_report . file ) } queue_message = json . dumps ( azure_dict ) message_bytes = queue_message . encode ( 'ascii' ) base64_bytes = base64 . b64encode ( message_bytes ) base64_message = base64_bytes . decode ( 'ascii' ) queue = QueueClient . from_connection_string ( conn_str =< connection string > , queue_name =< queue name > ) queue . send_message ( base64_message ) Figure 1 Triggering the Queue when uploading a Scan Report","title":"Triggering the function"},{"location":"AzureFunctions/processQueue/#accessing-the-file","text":"As soon as the ProcessQueue is triggered it uses the blob_name from the message body to download the file from Azure Blob Storage into a BytesIO() object.","title":"Accessing the File"},{"location":"AzureFunctions/processQueue/#processing","text":"The next step is to step through the Excel workbook and create model entries for each ScanReportTable , ScanReportField and ScanReportValue present in the file.","title":"Processing"},{"location":"AzureFunctions/processQueue/#scan-report-table","text":"The Field Overview sheet in a White Rabbit Scan Report has a 'Table' column which seperates different table names using a blank row. ProcessQueue main method iterates through the rows in the 'Table' column and appends each unique table name to the list table_names . For each table in table_names a new ScanReportTable entry is generated. The table entries are linked to the ScanReport model using the scan_report_id from the queue message body. The table entries are then appended to a JSON array json_data which forms the input send in a POST request to the API . The API automatically generates the id for each table that was created. These ids are extracted from the POST request response and saved in a list ( table_ids )","title":"Scan Report Table"},{"location":"AzureFunctions/processQueue/#scan-report-field","text":"Field Overview contains all the information on the fields including the name, description and other useful columns. ProcessQueue iterates through the rows in the sheet and generates a new ScanReportField entry. The field entries are linked to the relevant ScanReportTable using the previously generated table_ids . As soon as it detects an empty line (different table) it links to the next table_id from the list. Similarly as in Scan Report Table the field entries are appended to a JSON array json_data , and form the input to a POST request made to the API . From the response it saves the field_ids and field_names and stores them into a dictionary as key value pairs e.g \"Field Name\": Field ID.","title":"Scan Report Field"},{"location":"AzureFunctions/processQueue/#scan-report-value","text":"Scan report values are stored in sheets named after their corresponding table. For every sheet after the initial two 'Field Overview' and 'Table Overview'(not currently used), process_scan_report_sheet_table() is called. The function extracts the data in the following format: Input ID Frequency Date Frequency 1 20 02/12/2020 5 2 3 12/11/2020 37 Output (ID, 1, 20) (Date, 02/12/2020, 5) (ID, 2, 3) (Date, 12/11/2020, 37) A ScanReportValue entry is generated by iterating through the output of process_scan_report_sheet_table() . The value entries are linked to the ScanReportField model using the function output and the dictionary with all the field names and ids. Same as with the other two models the value entries are appended to a JSON array json_data , and form the input to a POST request made to the API .","title":"Scan Report Value"},{"location":"CoConnectTools/Common/","text":"coconnect.cdm.objects.common.DestinationTable \u00b6 Common object that all CDM objects (tables) inherit from __init__ ( self , _type , _version = 'v5_3_1' ) special \u00b6 Initialise the CDM DestinationTable Object class Parameters: Name Type Description Default _type str the name of the object being initialsed, e.g. \"person\" required _version str the CDM version, see https://github.com/OHDSI/CommonDataModel/tags 'v5_3_1' Returns: None Source code in coconnect/cdm/objects/common.py def __init__ ( self , _type , _version = 'v5_3_1' ): \"\"\" Initialise the CDM DestinationTable Object class Args: _type (str): the name of the object being initialsed, e.g. \"person\" _version (str): the CDM version, see https://github.com/OHDSI/CommonDataModel/tags Returns: None \"\"\" self . name = _type self . _type = _type self . _meta = {} self . logger = Logger ( self . name ) self . dtypes = DataFormatter () self . fields = self . get_field_names () if len ( self . fields ) == 0 : raise Exception ( \"something misconfigured - cannot find any DataTypes for {self.name} \" ) #print a check to see what cdm objects have been initialised self . logger . debug ( self . get_destination_fields ()) self . __df = None define ( self , _ ) \u00b6 define function, expected to be overloaded by the user defining the object Source code in coconnect/cdm/objects/common.py def define ( self , _ ): \"\"\" define function, expected to be overloaded by the user defining the object \"\"\" pass execute ( self , that ) \u00b6 execute the creation of the cdm object by passing Parameters: Name Type Description Default that input object class where input objects can be loaded and the define/finalise functions can be overloaded required Source code in coconnect/cdm/objects/common.py def execute ( self , that ): \"\"\" execute the creation of the cdm object by passing Args: that: input object class where input objects can be loaded and the define/finalise functions can be overloaded \"\"\" #extract all objects from the passed object objs = { k : v for k , v in that . __dict__ . items () if k != 'logger' } #add objects to this class self . __dict__ . update ( objs ) #execute the define function #the default define() does nothing #this is only executed if the CDM has been build via decorators #or define functions have been specified for this object # it will build the inputs from these functions self . define ( self ) #build the dataframe for this object _ = self . get_df () finalise ( self , df ) \u00b6 Finalise function, expected to be overloaded by children classes Source code in coconnect/cdm/objects/common.py def finalise ( self , df ): \"\"\" Finalise function, expected to be overloaded by children classes \"\"\" required_fields = [ field for field in self . get_field_names () if getattr ( self , field ) . required == True ] self . _meta [ 'required_fields' ] = {} for field in required_fields : nbefore = len ( df ) df = df [ ~ df [ field ] . isna ()] nafter = len ( df ) ndiff = nbefore - nafter if ndiff > 0 : self . logger . warning ( f \"Requiring non-null values in { field } removed { ndiff } rows, leaving { nafter } rows.\" ) self . _meta [ 'required_fields' ][ field ] = { 'before' : nbefore , 'after' : nafter } #if 'concept_id' in field: # values = df[field].unique().tolist() # self._meta['required_fields'][field]['concept_values'] = values df = df . sort_values ( self . get_ordering ()) return df format ( self , df ) \u00b6 Source code in coconnect/cdm/objects/common.py def format ( self , df ): for col in df . columns : obj = getattr ( self , col ) dtype = obj . dtype formatter_function = self . dtypes [ dtype ] df [ col ] = formatter_function ( df [ col ]) return df get_destination_fields ( self ) \u00b6 Get a list of all the destination fields that have been loaded and associated to this cdm object Returns: list: a list of all the destination fields that have been defined Source code in coconnect/cdm/objects/common.py def get_destination_fields ( self ): \"\"\" Get a list of all the destination fields that have been loaded and associated to this cdm object Returns: list: a list of all the destination fields that have been defined \"\"\" return list ( self . fields ) get_df ( self , force_rebuild = False ) \u00b6 Retrieve a dataframe from the current object Returns: Type Description pandas.Dataframe extracted dataframe of the cdm object Source code in coconnect/cdm/objects/common.py def get_df ( self , force_rebuild = False ): \"\"\" Retrieve a dataframe from the current object Returns: pandas.Dataframe: extracted dataframe of the cdm object \"\"\" #if the dataframe has already been built.. just return it if not self . __df is None and not force_rebuild : return self . __df #get a dict of all series #each object is a pandas series dfs = {} for field in self . fields : obj = getattr ( self , field ) series = obj . series if series is None : #if required: # self.logger.error(f\"{field} is all null/none or has not been set/defined\") # raise RequiredFieldIsNone(f\"{field} is a required for {self.name}.\") continue #rename the column to be the final destination field name series = series . rename ( field ) #register the new series dfs [ field ] = series #if there's none defined, dont do anything if len ( dfs ) == 0 : return None #check the lengths of the dataframes lengths = list ( set ([ len ( df ) for df in dfs . values ()])) if len ( lengths ) > 1 : self . logger . error ( \"One or more inputs being mapped to this object has a different number of entries\" ) for name , df in dfs . items (): self . logger . error ( f \" { name } of length { len ( df ) } \" ) raise BadInputs ( \"Differring number of rows in the inputs\" ) #create a dataframe from all the series objects df = pd . concat ( dfs . values (), axis = 1 ) #find which fields in the cdm havent been defined missing_fields = set ( self . fields ) - set ( df . columns ) self . _meta [ 'defined_columns' ] = df . columns . tolist () self . _meta [ 'undefined_columns' ] = list ( missing_fields ) #set these to a nan/null series for field in missing_fields : df [ field ] = np . NaN #simply order the columns df = df [ self . fields ] df = self . finalise ( df ) df = self . format ( df ) #register the df self . __df = df return df get_field_names ( self ) \u00b6 Source code in coconnect/cdm/objects/common.py def get_field_names ( self ): return [ item for item in self . __dict__ . keys () if isinstance ( getattr ( self , item ), DestinationField ) ] get_ordering ( self ) \u00b6 Source code in coconnect/cdm/objects/common.py def get_ordering ( self ): return [ field for field in self . fields if getattr ( self , field ) . pk == True ] set_name ( self , name ) \u00b6 Source code in coconnect/cdm/objects/common.py def set_name ( self , name ): self . name = name self . logger . name = self . name","title":"Common"},{"location":"CoConnectTools/Common/#coconnect.cdm.objects.common.DestinationTable","text":"Common object that all CDM objects (tables) inherit from","title":"DestinationTable"},{"location":"CoConnectTools/Common/#coconnect.cdm.objects.common.DestinationTable.__init__","text":"Initialise the CDM DestinationTable Object class Parameters: Name Type Description Default _type str the name of the object being initialsed, e.g. \"person\" required _version str the CDM version, see https://github.com/OHDSI/CommonDataModel/tags 'v5_3_1' Returns: None Source code in coconnect/cdm/objects/common.py def __init__ ( self , _type , _version = 'v5_3_1' ): \"\"\" Initialise the CDM DestinationTable Object class Args: _type (str): the name of the object being initialsed, e.g. \"person\" _version (str): the CDM version, see https://github.com/OHDSI/CommonDataModel/tags Returns: None \"\"\" self . name = _type self . _type = _type self . _meta = {} self . logger = Logger ( self . name ) self . dtypes = DataFormatter () self . fields = self . get_field_names () if len ( self . fields ) == 0 : raise Exception ( \"something misconfigured - cannot find any DataTypes for {self.name} \" ) #print a check to see what cdm objects have been initialised self . logger . debug ( self . get_destination_fields ()) self . __df = None","title":"__init__()"},{"location":"CoConnectTools/Common/#coconnect.cdm.objects.common.DestinationTable.define","text":"define function, expected to be overloaded by the user defining the object Source code in coconnect/cdm/objects/common.py def define ( self , _ ): \"\"\" define function, expected to be overloaded by the user defining the object \"\"\" pass","title":"define()"},{"location":"CoConnectTools/Common/#coconnect.cdm.objects.common.DestinationTable.execute","text":"execute the creation of the cdm object by passing Parameters: Name Type Description Default that input object class where input objects can be loaded and the define/finalise functions can be overloaded required Source code in coconnect/cdm/objects/common.py def execute ( self , that ): \"\"\" execute the creation of the cdm object by passing Args: that: input object class where input objects can be loaded and the define/finalise functions can be overloaded \"\"\" #extract all objects from the passed object objs = { k : v for k , v in that . __dict__ . items () if k != 'logger' } #add objects to this class self . __dict__ . update ( objs ) #execute the define function #the default define() does nothing #this is only executed if the CDM has been build via decorators #or define functions have been specified for this object # it will build the inputs from these functions self . define ( self ) #build the dataframe for this object _ = self . get_df ()","title":"execute()"},{"location":"CoConnectTools/Common/#coconnect.cdm.objects.common.DestinationTable.finalise","text":"Finalise function, expected to be overloaded by children classes Source code in coconnect/cdm/objects/common.py def finalise ( self , df ): \"\"\" Finalise function, expected to be overloaded by children classes \"\"\" required_fields = [ field for field in self . get_field_names () if getattr ( self , field ) . required == True ] self . _meta [ 'required_fields' ] = {} for field in required_fields : nbefore = len ( df ) df = df [ ~ df [ field ] . isna ()] nafter = len ( df ) ndiff = nbefore - nafter if ndiff > 0 : self . logger . warning ( f \"Requiring non-null values in { field } removed { ndiff } rows, leaving { nafter } rows.\" ) self . _meta [ 'required_fields' ][ field ] = { 'before' : nbefore , 'after' : nafter } #if 'concept_id' in field: # values = df[field].unique().tolist() # self._meta['required_fields'][field]['concept_values'] = values df = df . sort_values ( self . get_ordering ()) return df","title":"finalise()"},{"location":"CoConnectTools/Common/#coconnect.cdm.objects.common.DestinationTable.format","text":"Source code in coconnect/cdm/objects/common.py def format ( self , df ): for col in df . columns : obj = getattr ( self , col ) dtype = obj . dtype formatter_function = self . dtypes [ dtype ] df [ col ] = formatter_function ( df [ col ]) return df","title":"format()"},{"location":"CoConnectTools/Common/#coconnect.cdm.objects.common.DestinationTable.get_destination_fields","text":"Get a list of all the destination fields that have been loaded and associated to this cdm object Returns: list: a list of all the destination fields that have been defined Source code in coconnect/cdm/objects/common.py def get_destination_fields ( self ): \"\"\" Get a list of all the destination fields that have been loaded and associated to this cdm object Returns: list: a list of all the destination fields that have been defined \"\"\" return list ( self . fields )","title":"get_destination_fields()"},{"location":"CoConnectTools/Common/#coconnect.cdm.objects.common.DestinationTable.get_df","text":"Retrieve a dataframe from the current object Returns: Type Description pandas.Dataframe extracted dataframe of the cdm object Source code in coconnect/cdm/objects/common.py def get_df ( self , force_rebuild = False ): \"\"\" Retrieve a dataframe from the current object Returns: pandas.Dataframe: extracted dataframe of the cdm object \"\"\" #if the dataframe has already been built.. just return it if not self . __df is None and not force_rebuild : return self . __df #get a dict of all series #each object is a pandas series dfs = {} for field in self . fields : obj = getattr ( self , field ) series = obj . series if series is None : #if required: # self.logger.error(f\"{field} is all null/none or has not been set/defined\") # raise RequiredFieldIsNone(f\"{field} is a required for {self.name}.\") continue #rename the column to be the final destination field name series = series . rename ( field ) #register the new series dfs [ field ] = series #if there's none defined, dont do anything if len ( dfs ) == 0 : return None #check the lengths of the dataframes lengths = list ( set ([ len ( df ) for df in dfs . values ()])) if len ( lengths ) > 1 : self . logger . error ( \"One or more inputs being mapped to this object has a different number of entries\" ) for name , df in dfs . items (): self . logger . error ( f \" { name } of length { len ( df ) } \" ) raise BadInputs ( \"Differring number of rows in the inputs\" ) #create a dataframe from all the series objects df = pd . concat ( dfs . values (), axis = 1 ) #find which fields in the cdm havent been defined missing_fields = set ( self . fields ) - set ( df . columns ) self . _meta [ 'defined_columns' ] = df . columns . tolist () self . _meta [ 'undefined_columns' ] = list ( missing_fields ) #set these to a nan/null series for field in missing_fields : df [ field ] = np . NaN #simply order the columns df = df [ self . fields ] df = self . finalise ( df ) df = self . format ( df ) #register the df self . __df = df return df","title":"get_df()"},{"location":"CoConnectTools/Common/#coconnect.cdm.objects.common.DestinationTable.get_field_names","text":"Source code in coconnect/cdm/objects/common.py def get_field_names ( self ): return [ item for item in self . __dict__ . keys () if isinstance ( getattr ( self , item ), DestinationField ) ]","title":"get_field_names()"},{"location":"CoConnectTools/Common/#coconnect.cdm.objects.common.DestinationTable.get_ordering","text":"Source code in coconnect/cdm/objects/common.py def get_ordering ( self ): return [ field for field in self . fields if getattr ( self , field ) . pk == True ]","title":"get_ordering()"},{"location":"CoConnectTools/Common/#coconnect.cdm.objects.common.DestinationTable.set_name","text":"Source code in coconnect/cdm/objects/common.py def set_name ( self , name ): self . name = name self . logger . name = self . name","title":"set_name()"},{"location":"CoConnectTools/CommonDataModel/","text":"The Pythonic version of the CommonDataModel is built object-orientated in the subfolder coconnect/cdm/ : coconnect/cdm/ \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 classes \u2502 \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 decorators.py \u251c\u2500\u2500 model.py \u251c\u2500\u2500 objects \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 common.py \u2502 \u251c\u2500\u2500 condition_occurrence.py \u2502 \u251c\u2500\u2500 measurement.py \u2502 \u251c\u2500\u2500 observation.py \u2502 \u251c\u2500\u2500 person.py \u2502 \u2514\u2500\u2500 visit_occurrence.py \u2514\u2500\u2500 operations.py Destination Tables \u00b6 All CDM destination tables are formed as objects and are defined in coconnect/cdm/objects , inheriting from a base class ( DestinationTable , defined in common.py ): Person Condition Occurrence Observation Measurement Generating More Tables \u00b6 The package contains .csv files taken from https://github.com/OHDSI/CommonDataModel/tags that give descriptions of what fields are contained within each CDM table. At present there is just OMOP_CDM_v5_3_1.csv from this url that is present: $ ls $(coconnect info data_folder)/cdm/ OMOP_CDM_ONLINE_LATEST.csv OMOP_CDM_v5_3_1.csv Danger There are known inconstitencies with the .csv files that are stored on the OHDSI GitHub. Info For this reason we store the file OMOP_CDM_ONLINE_LATEST.csv in the package. This is a csv file taken from a raw dump of the latest OMOP CDM that the coconnect team are using. At this moment in time, it is version 5.3.1, and slightly differs from what should be the v5.3.1 in OMOP_CDM_v5_3_1.csv To help generate a pythonic template for a CDM template, the CLI can be used to do this $ coconnect generate cdm drug_exposure 5.3.1 This command tool outputs the following code that you could use to copy, paster and edit to create a new table for drug_exposure self . drug_exposure_id = DestinationField ( dtype = \"INTEGER\" , required = True ) self . person_id = DestinationField ( dtype = \"INTEGER\" , required = True ) self . drug_concept_id = DestinationField ( dtype = \"INTEGER\" , required = True ) self . drug_exposure_start_date = DestinationField ( dtype = \"DATE\" , required = True ) self . drug_exposure_start_datetime = DestinationField ( dtype = \"DATETIME\" , required = False ) self . drug_exposure_end_date = DestinationField ( dtype = \"DATE\" , required = True ) self . drug_exposure_end_datetime = DestinationField ( dtype = \"DATETIME\" , required = False ) self . verbatim_end_date = DestinationField ( dtype = \"DATE\" , required = False ) self . drug_type_concept_id = DestinationField ( dtype = \"INTEGER\" , required = True ) self . stop_reason = DestinationField ( dtype = \"VARCHAR(20)\" , required = False ) self . refills = DestinationField ( dtype = \"INTEGER\" , required = False ) self . quantity = DestinationField ( dtype = \"FLOAT\" , required = False ) self . days_supply = DestinationField ( dtype = \"INTEGER\" , required = False ) self . sig = DestinationField ( dtype = \"VARCHAR(MAX)\" , required = False ) self . route_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . lot_number = DestinationField ( dtype = \"VARCHAR(50)\" , required = False ) self . provider_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . visit_occurrence_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . visit_detail_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . drug_source_value = DestinationField ( dtype = \"VARCHAR(50)\" , required = False ) self . drug_source_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . route_source_value = DestinationField ( dtype = \"VARCHAR(50)\" , required = False ) self . dose_unit_source_value = DestinationField ( dtype = \"VARCHAR(50)\" , required = False ) Destination Fields \u00b6 In common.py a class called DestinationField defines how to handle an input pandas series. This pandas series is effectively a column in the output of the CDM Tables, in other words, DestinationField is an object for the destination_field , e.g. person_id in the destination_table person . class DestinationField ( object ): def __init__ ( self , dtype : str , required : bool , pk = False ): self . series = None self . dtype = dtype self . required = required self . pk = pk self.series : initialises as None and will hold a pandas.Series object if the column is to be filled in the output. self.dtype : specifies a string for handling how to format the output of this column so it's in the right format when saved to the final .csv to be uploaded successfully to a BCLink. self.required : if the destination_field is required ( True ), any rows of the final table that do not have this column filled ( None , NaN ), are removed (dropped) from the output. self.pk : specifies if this column is the primary key for its associated table. This can be used to order the tables based on this.","title":"Overview"},{"location":"CoConnectTools/CommonDataModel/#destination-tables","text":"All CDM destination tables are formed as objects and are defined in coconnect/cdm/objects , inheriting from a base class ( DestinationTable , defined in common.py ): Person Condition Occurrence Observation Measurement","title":"Destination Tables"},{"location":"CoConnectTools/CommonDataModel/#generating-more-tables","text":"The package contains .csv files taken from https://github.com/OHDSI/CommonDataModel/tags that give descriptions of what fields are contained within each CDM table. At present there is just OMOP_CDM_v5_3_1.csv from this url that is present: $ ls $(coconnect info data_folder)/cdm/ OMOP_CDM_ONLINE_LATEST.csv OMOP_CDM_v5_3_1.csv Danger There are known inconstitencies with the .csv files that are stored on the OHDSI GitHub. Info For this reason we store the file OMOP_CDM_ONLINE_LATEST.csv in the package. This is a csv file taken from a raw dump of the latest OMOP CDM that the coconnect team are using. At this moment in time, it is version 5.3.1, and slightly differs from what should be the v5.3.1 in OMOP_CDM_v5_3_1.csv To help generate a pythonic template for a CDM template, the CLI can be used to do this $ coconnect generate cdm drug_exposure 5.3.1 This command tool outputs the following code that you could use to copy, paster and edit to create a new table for drug_exposure self . drug_exposure_id = DestinationField ( dtype = \"INTEGER\" , required = True ) self . person_id = DestinationField ( dtype = \"INTEGER\" , required = True ) self . drug_concept_id = DestinationField ( dtype = \"INTEGER\" , required = True ) self . drug_exposure_start_date = DestinationField ( dtype = \"DATE\" , required = True ) self . drug_exposure_start_datetime = DestinationField ( dtype = \"DATETIME\" , required = False ) self . drug_exposure_end_date = DestinationField ( dtype = \"DATE\" , required = True ) self . drug_exposure_end_datetime = DestinationField ( dtype = \"DATETIME\" , required = False ) self . verbatim_end_date = DestinationField ( dtype = \"DATE\" , required = False ) self . drug_type_concept_id = DestinationField ( dtype = \"INTEGER\" , required = True ) self . stop_reason = DestinationField ( dtype = \"VARCHAR(20)\" , required = False ) self . refills = DestinationField ( dtype = \"INTEGER\" , required = False ) self . quantity = DestinationField ( dtype = \"FLOAT\" , required = False ) self . days_supply = DestinationField ( dtype = \"INTEGER\" , required = False ) self . sig = DestinationField ( dtype = \"VARCHAR(MAX)\" , required = False ) self . route_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . lot_number = DestinationField ( dtype = \"VARCHAR(50)\" , required = False ) self . provider_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . visit_occurrence_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . visit_detail_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . drug_source_value = DestinationField ( dtype = \"VARCHAR(50)\" , required = False ) self . drug_source_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . route_source_value = DestinationField ( dtype = \"VARCHAR(50)\" , required = False ) self . dose_unit_source_value = DestinationField ( dtype = \"VARCHAR(50)\" , required = False )","title":"Generating More Tables"},{"location":"CoConnectTools/CommonDataModel/#destination-fields","text":"In common.py a class called DestinationField defines how to handle an input pandas series. This pandas series is effectively a column in the output of the CDM Tables, in other words, DestinationField is an object for the destination_field , e.g. person_id in the destination_table person . class DestinationField ( object ): def __init__ ( self , dtype : str , required : bool , pk = False ): self . series = None self . dtype = dtype self . required = required self . pk = pk self.series : initialises as None and will hold a pandas.Series object if the column is to be filled in the output. self.dtype : specifies a string for handling how to format the output of this column so it's in the right format when saved to the final .csv to be uploaded successfully to a BCLink. self.required : if the destination_field is required ( True ), any rows of the final table that do not have this column filled ( None , NaN ), are removed (dropped) from the output. self.pk : specifies if this column is the primary key for its associated table. This can be used to order the tables based on this.","title":"Destination Fields"},{"location":"CoConnectTools/ConditionOccurrence/","text":"class ConditionOccurrence ( DestinationTable ): \"\"\" CDM Condition Occurrence object class \"\"\" name = 'condition_occurrence' def __init__ ( self ): self . condition_occurrence_id = DestinationField ( dtype = \"INTEGER\" , required = True , pk = True ) self . person_id = DestinationField ( dtype = \"INTEGER\" , required = True ) self . condition_concept_id = DestinationField ( dtype = \"INTEGER\" , required = True ) self . condition_start_date = DestinationField ( dtype = \"DATE\" , required = False ) self . condition_start_datetime = DestinationField ( dtype = \"DATETIME\" , required = True ) self . condition_end_date = DestinationField ( dtype = \"DATE\" , required = False ) self . condition_end_datetime = DestinationField ( dtype = \"DATETIME\" , required = False ) self . condition_type_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . stop_reason = DestinationField ( dtype = \"VARCHAR(20)\" , required = False ) self . provider_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . visit_occurrence_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . visit_detail_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . condition_source_value = DestinationField ( dtype = \"VARCHAR(50)\" , required = False ) self . condition_source_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . condition_status_source_value = DestinationField ( dtype = \"VARCHAR(50)\" , required = False ) self . condition_status_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False )","title":"Condition Occurrence"},{"location":"CoConnectTools/Measurement/","text":"class Measurement ( DestinationTable ): \"\"\" CDM Measurement object class \"\"\" name = 'measurement' def __init__ ( self ): self . measurement_id = DestinationField ( dtype = \"INTEGER\" , required = True , pk = True ) self . person_id = DestinationField ( dtype = \"INTEGER\" , required = True ) self . measurement_concept_id = DestinationField ( dtype = \"INTEGER\" , required = True ) self . measurement_date = DestinationField ( dtype = \"DATE\" , required = False ) self . measurement_datetime = DestinationField ( dtype = \"DATETIME\" , required = True ) self . measurement_time = DestinationField ( dtype = \"VARCHAR(10)\" , required = False ) self . measurement_type_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . operator_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . value_as_number = DestinationField ( dtype = \"FLOAT\" , required = False ) self . value_as_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . unit_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . range_low = DestinationField ( dtype = \"FLOAT\" , required = False ) self . range_high = DestinationField ( dtype = \"FLOAT\" , required = False ) self . provider_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . visit_occurrence_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . visit_detail_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . measurement_source_value = DestinationField ( dtype = \"VARCHAR(50)\" , required = False ) self . measurement_source_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . unit_source_value = DestinationField ( dtype = \"VARCHAR(50)\" , required = False ) self . value_source_value = DestinationField ( dtype = \"VARCHAR(50)\" , required = False )","title":"Measurement"},{"location":"CoConnectTools/Observation/","text":"class Observation ( DestinationTable ): \"\"\" CDM Observation object class \"\"\" name = 'observation' def __init__ ( self ): self . observation_id = DestinationField ( dtype = \"INTEGER\" , required = True , pk = True ) self . person_id = DestinationField ( dtype = \"INTEGER\" , required = True ) self . observation_concept_id = DestinationField ( dtype = \"INTEGER\" , required = True ) self . observation_date = DestinationField ( dtype = \"DATE\" , required = False ) self . observation_datetime = DestinationField ( dtype = \"DATETIME\" , required = True ) self . observation_type_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . value_as_number = DestinationField ( dtype = \"FLOAT\" , required = False ) self . value_as_string = DestinationField ( dtype = \"VARCHAR(60)\" , required = False ) self . value_as_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . qualifier_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . unit_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . provider_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . visit_occurrence_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . visit_detail_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . observation_source_value = DestinationField ( dtype = \"VARCHAR(50)\" , required = False ) self . observation_source_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . unit_source_value = DestinationField ( dtype = \"VARCHAR(50)\" , required = False ) self . qualifier_source_value = DestinationField ( dtype = \"VARCHAR(50)\" , required = False )","title":"Observation"},{"location":"CoConnectTools/Person/","text":"coconnect.cdm.objects.person.Person \u00b6 CDM Person object class name \u00b6 __init__ ( self ) special \u00b6 Source code in coconnect/cdm/objects/person.py def __init__ ( self ): self . person_id = DestinationField ( dtype = \"INTEGER\" , required = True , pk = True ) self . gender_concept_id = DestinationField ( dtype = \"INTEGER\" , required = True ) self . year_of_birth = DestinationField ( dtype = \"INTEGER\" , required = False ) self . month_of_birth = DestinationField ( dtype = \"INTEGER\" , required = False ) self . day_of_birth = DestinationField ( dtype = \"INTEGER\" , required = False ) self . birth_datetime = DestinationField ( dtype = \"DATETIME\" , required = True ) self . race_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . ethnicity_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . location_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . provider_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . care_site_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . person_source_value = DestinationField ( dtype = \"VARCHAR(50)\" , required = False ) self . gender_source_value = DestinationField ( dtype = \"VARCHAR(50)\" , required = False ) self . gender_source_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . race_source_value = DestinationField ( dtype = \"VARCHAR(50)\" , required = False ) self . race_source_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . ethnicity_source_value = DestinationField ( dtype = \"VARCHAR(50)\" , required = False ) self . ethnicity_source_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False ) super () . __init__ ( self . name ) finalise ( self , df ) \u00b6 Overload the finalise function here for any specifics for the person table Source code in coconnect/cdm/objects/person.py def finalise ( self , df ): \"\"\" Overload the finalise function here for any specifics for the person table \"\"\" df = super () . finalise ( df ) return df get_df ( self ) \u00b6 Overload/append the creation of the dataframe, specifically for the person objects * year_of_birth is automatically converted to a year (int) * month_of_birth is automatically converted to a month (int) * day_of_birth is automatically converted to a day (int) * birth_datetime is automatically coverted to a datatime Returns: Type Description pandas.Dataframe output dataframe Source code in coconnect/cdm/objects/person.py def get_df ( self ): \"\"\" Overload/append the creation of the dataframe, specifically for the person objects * year_of_birth is automatically converted to a year (int) * month_of_birth is automatically converted to a month (int) * day_of_birth is automatically converted to a day (int) * birth_datetime is automatically coverted to a datatime Returns: pandas.Dataframe: output dataframe \"\"\" df = super () . get_df () if self . automatically_generate_missing_rules == True : if df [ 'year_of_birth' ] . isnull () . all (): df [ 'year_of_birth' ] = self . tools . get_year ( df [ 'birth_datetime' ]) if df [ 'month_of_birth' ] . isnull () . all (): df [ 'month_of_birth' ] = self . tools . get_month ( df [ 'birth_datetime' ]) if df [ 'day_of_birth' ] . isnull () . all (): df [ 'day_of_birth' ] = self . tools . get_day ( df [ 'birth_datetime' ]) return df","title":"Person"},{"location":"CoConnectTools/Person/#coconnect.cdm.objects.person.Person","text":"CDM Person object class","title":"Person"},{"location":"CoConnectTools/Person/#coconnect.cdm.objects.person.Person.name","text":"","title":"name"},{"location":"CoConnectTools/Person/#coconnect.cdm.objects.person.Person.__init__","text":"Source code in coconnect/cdm/objects/person.py def __init__ ( self ): self . person_id = DestinationField ( dtype = \"INTEGER\" , required = True , pk = True ) self . gender_concept_id = DestinationField ( dtype = \"INTEGER\" , required = True ) self . year_of_birth = DestinationField ( dtype = \"INTEGER\" , required = False ) self . month_of_birth = DestinationField ( dtype = \"INTEGER\" , required = False ) self . day_of_birth = DestinationField ( dtype = \"INTEGER\" , required = False ) self . birth_datetime = DestinationField ( dtype = \"DATETIME\" , required = True ) self . race_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . ethnicity_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . location_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . provider_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . care_site_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . person_source_value = DestinationField ( dtype = \"VARCHAR(50)\" , required = False ) self . gender_source_value = DestinationField ( dtype = \"VARCHAR(50)\" , required = False ) self . gender_source_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . race_source_value = DestinationField ( dtype = \"VARCHAR(50)\" , required = False ) self . race_source_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False ) self . ethnicity_source_value = DestinationField ( dtype = \"VARCHAR(50)\" , required = False ) self . ethnicity_source_concept_id = DestinationField ( dtype = \"INTEGER\" , required = False ) super () . __init__ ( self . name )","title":"__init__()"},{"location":"CoConnectTools/Person/#coconnect.cdm.objects.person.Person.finalise","text":"Overload the finalise function here for any specifics for the person table Source code in coconnect/cdm/objects/person.py def finalise ( self , df ): \"\"\" Overload the finalise function here for any specifics for the person table \"\"\" df = super () . finalise ( df ) return df","title":"finalise()"},{"location":"CoConnectTools/Person/#coconnect.cdm.objects.person.Person.get_df","text":"Overload/append the creation of the dataframe, specifically for the person objects * year_of_birth is automatically converted to a year (int) * month_of_birth is automatically converted to a month (int) * day_of_birth is automatically converted to a day (int) * birth_datetime is automatically coverted to a datatime Returns: Type Description pandas.Dataframe output dataframe Source code in coconnect/cdm/objects/person.py def get_df ( self ): \"\"\" Overload/append the creation of the dataframe, specifically for the person objects * year_of_birth is automatically converted to a year (int) * month_of_birth is automatically converted to a month (int) * day_of_birth is automatically converted to a day (int) * birth_datetime is automatically coverted to a datatime Returns: pandas.Dataframe: output dataframe \"\"\" df = super () . get_df () if self . automatically_generate_missing_rules == True : if df [ 'year_of_birth' ] . isnull () . all (): df [ 'year_of_birth' ] = self . tools . get_year ( df [ 'birth_datetime' ]) if df [ 'month_of_birth' ] . isnull () . all (): df [ 'month_of_birth' ] = self . tools . get_month ( df [ 'birth_datetime' ]) if df [ 'day_of_birth' ] . isnull () . all (): df [ 'day_of_birth' ] = self . tools . get_day ( df [ 'birth_datetime' ]) return df","title":"get_df()"},{"location":"CoConnectTools/source_code/","text":"CO-CONNECT Tools \u00b6 Welcome to our repo for python tools used by/with the CO-CONNECT project Table of Contents \u00b6 Installing ETL Quick Start CLI Installing \u00b6 This package runs with python versions >=3.6 , the easiest way to install is via pip: $ pip install co-connect-tools To ensure the right version of python (python3) is used to install from pip you can do: $ pip3 install co-connect-tools Or to be even safer: $ python3 -m pip install co-connect-tools From Source \u00b6 To install from the source code, you can do: $ git clone https://github.com/CO-CONNECT/co-connect-tools.git $ cd co-connect-tools $ pip install -e . Installing via yum \u00b6 Package dependencies are located in requirements.txt , which need to be installed if you are building from source without pip: $ cat requirements.txt numpy pandas coloredlogs Jinja2 graphviz click sqlalchemy tabulate ETL Quick Start \u00b6 The primary purpose of this package is running ETL of given a dataset and a set of transform rules encoded within a json file. The simplest way to run the ETLTool, designed to handle the output json of the CO-CONNECT Mapping-Pipeline web-tool, is to use the script etlcdm.py : After installing the package you should be able to execute the command line script $ etlcdm.py --help usage: etlcdm.py [-h] --rules RULES --out-dir OUT_DIR --inputs INPUTS [INPUTS ...] ETL-CDM: transform a dataset into a CommonDataModel. optional arguments: -h, --help show this help message and exit --rules RULES input .json file --out-dir OUT_DIR, -o OUT_DIR name of the output folder --inputs INPUTS [INPUTS ...], -i INPUTS [INPUTS ...] input csv files Setup \u00b6 To run this example, obtain the location of the coconnect data folder, and set this as an environment variable for ease. export COCONNECT_DATA_FOLDER=$(coconnect info data_folder) Execute \u00b6 The example dataset and associated mapping rules can be run with the simple script etlcdm.py : etlcdm.py -i $COCONNECT_DATA_FOLDER /test/inputs/*.csv --rules $COCONNECT_DATA_FOLDER /test/rules/rules_14June2021.json -o test/ Inspecting the Output \u00b6 .csv files are created for each CDM table, e.g. person.csv . Additionally logs are created and stored in the sub-folder logs/ . $ tree test/ test/ \u251c\u2500\u2500 condition_occurrence.csv \u251c\u2500\u2500 logs \u2502 \u2514\u2500\u2500 2021-06-16T100657.json \u251c\u2500\u2500 observation.csv \u2514\u2500\u2500 person.csv A convenience command exists to be able to display the output dataframe to the command-line: $ coconnect display dataframe test/person.csv person_id gender_concept_id ... ethnicity_source_value ethnicity_source_concept_id 0 101 8507 ... NaN NaN 1 102 8507 ... NaN NaN 2 103 8532 ... NaN NaN 3 104 8532 ... NaN NaN 4 105 8532 ... NaN NaN 5 106 8507 ... NaN NaN 6 107 8532 ... NaN NaN 7 108 8507 ... NaN NaN 8 109 8532 ... NaN NaN 9 110 8532 ... NaN NaN [10 rows x 18 columns] This can also be used with the option --drop-na to just display those columns which have none-NaN values $ coconnect display dataframe --drop-na test/person.csv person_id gender_concept_id birth_datetime gender_source_value gender_source_concept_id 0 101 8507 1951-12-25 00:00:00 M 8507 1 102 8507 1981-11-19 00:00:00 M 8507 2 103 8532 1997-05-11 00:00:00 F 8532 3 104 8532 1975-06-07 00:00:00 F 8532 4 105 8532 1976-04-23 00:00:00 F 8532 5 106 8507 1966-09-29 00:00:00 M 8507 6 107 8532 1956-11-12 00:00:00 F 8532 7 108 8507 1985-03-01 00:00:00 M 8507 8 109 8532 1950-10-31 00:00:00 F 8532 9 110 8532 1993-09-07 00:00:00 F 8532 Markdown format can be obtained for convenience: $ coconnect display dataframe --markdown --drop-na test/person.csv person_id gender_concept_id birth_datetime gender_source_value gender_source_concept_id 0 101 8507 1951-12-25 00:00:00 M 8507 1 102 8507 1981-11-19 00:00:00 M 8507 2 103 8532 1997-05-11 00:00:00 F 8532 3 104 8532 1975-06-07 00:00:00 F 8532 4 105 8532 1976-04-23 00:00:00 F 8532 5 106 8507 1966-09-29 00:00:00 M 8507 6 107 8532 1956-11-12 00:00:00 F 8532 7 108 8507 1985-03-01 00:00:00 M 8507 8 109 8532 1950-10-31 00:00:00 F 8532 9 110 8532 1993-09-07 00:00:00 F 8532 Command Line Interface \u00b6 To list the available commands available with the cli , you can simply use --help : $ coconnect --help Usage: coconnect [ OPTIONS ] COMMAND [ ARGS ] ... Options: -l, --loglevel TEXT --help Show this message and exit. Commands: map Map \u00b6 Commands used for mapping datasets with OMOP are found with the command map $ coconnect map --help Usage: coconnect map [ OPTIONS ] COMMAND [ ARGS ] ... Options: --help Show this message and exit. Commands: display Display the OMOP mapping json as a DAG list List all the python classes there are available to run make Generate a python class from the OMOP mapping json run Perform OMOP Mapping show Show the OMOP mapping json Example \u00b6 This quick example shows how you can run OMOP mapping based on a sample json file Firstly run show to display the input json structure $ coconnect map show example/sample_config/lion_structural_mapping.json ... \"cdm\": { \"person\": [ { \"birth_datetime\": { \"source_table\": \"demo.csv\", \"source_field\": \"dob\", \"term_mapping\": null, \"operations\": [ \"get_datetime\" ] }, ... To display this json as a dag you can run: $ coconnect map display example/sample_config/lion_structural_mapping.json The next step is to create a .py configuration file from this input json . The tool automatically registers these files, to see registered files, you can run: $ coconnect map list {} Showing that no files have been created and registered. To create your first configuration file, run make specifying the name of the output file/class: $ coconnect map make --name Lion example/sample_config/lion_structural_mapping.json Making new file <your install dir>/co-connect-tools/coconnect/cdm/classes/Lion.py Looking at the file that has been created, you can see the .py configuration file has been made: from coconnect.cdm import define_person , define_condition_occurrence , define_visit_occurrence , define_measurement , define_observation from coconnect.cdm import CommonDataModel import json class Lion ( CommonDataModel ): ... @define_person def person_0 ( self ): \"\"\" Create CDM object for person \"\"\" self . birth_datetime = self . inputs [ \"demo.csv\" ][ \"dob\" ] self . day_of_birth = self . inputs [ \"demo.csv\" ][ \"dob\" ] self . gender_concept_id = self . inputs [ \"demo.csv\" ][ \"gender\" ] self . gender_source_concept_id = self . inputs [ \"demo.csv\" ][ \"gender\" ] self . gender_source_value = self . inputs [ \"demo.csv\" ][ \"gender\" ] self . month_of_birth = self . inputs [ \"demo.csv\" ][ \"dob\" ] .... Now the list command shows that the file has been registered with the tool: $ coconnect map list { \"Lion\": { \"module\": \"coconnect.cdm.classes.Lion\", \"path\": \"/Users/calummacdonald/Usher/CO-CONNECT/Software/co-connect-tools/coconnect/cdm/classes/Lion.py\", \"last-modified\": \"2021-04-16 10:26:33\" } } Now we're able to run: $ coconnect map run --name Lion example/sample_input_data/*.csv ... 2021-04-16 10:29:12 - Lion - INFO - finalised observation 2021-04-16 10:29:12 - Lion - INFO - saving person to output_data//person.csv 2021-04-16 10:29:12 - Lion - INFO - gender_concept_id year_of_birth ... race_source_concept_id ethnicity_source_value person_id ... 1 8532 1962 ... 123456 2 8507 1972 ... 123456 3 8532 1979 ... 123422 4 8532 1991 ... 123456 [4 rows x 12 columns] ... Outputs are saved in the folder output_data All in One \u00b6 In one command, all the above steps can be executed as such: Example: coconnect map run --name Lion --rules example/sample_config/lion_structural_mapping.json example/sample_input_data/*.csv To run in one command, supply the name of the dataset (e.g. Panther), the rules json file that has been obtained from mapping-pipeline and then all the input files to run on. coconnect map run --name <NAME> --rules <RULES.json> <INPUTFILE 1 > <INPUTFILE 2 > ....","title":"Getting Started"},{"location":"CoConnectTools/source_code/#co-connect-tools","text":"Welcome to our repo for python tools used by/with the CO-CONNECT project","title":"CO-CONNECT Tools"},{"location":"CoConnectTools/source_code/#table-of-contents","text":"Installing ETL Quick Start CLI","title":"Table of Contents"},{"location":"CoConnectTools/source_code/#installing","text":"This package runs with python versions >=3.6 , the easiest way to install is via pip: $ pip install co-connect-tools To ensure the right version of python (python3) is used to install from pip you can do: $ pip3 install co-connect-tools Or to be even safer: $ python3 -m pip install co-connect-tools","title":"Installing"},{"location":"CoConnectTools/source_code/#from-source","text":"To install from the source code, you can do: $ git clone https://github.com/CO-CONNECT/co-connect-tools.git $ cd co-connect-tools $ pip install -e .","title":"From Source"},{"location":"CoConnectTools/source_code/#installing-via-yum","text":"Package dependencies are located in requirements.txt , which need to be installed if you are building from source without pip: $ cat requirements.txt numpy pandas coloredlogs Jinja2 graphviz click sqlalchemy tabulate","title":"Installing via yum"},{"location":"CoConnectTools/source_code/#etl-quick-start","text":"The primary purpose of this package is running ETL of given a dataset and a set of transform rules encoded within a json file. The simplest way to run the ETLTool, designed to handle the output json of the CO-CONNECT Mapping-Pipeline web-tool, is to use the script etlcdm.py : After installing the package you should be able to execute the command line script $ etlcdm.py --help usage: etlcdm.py [-h] --rules RULES --out-dir OUT_DIR --inputs INPUTS [INPUTS ...] ETL-CDM: transform a dataset into a CommonDataModel. optional arguments: -h, --help show this help message and exit --rules RULES input .json file --out-dir OUT_DIR, -o OUT_DIR name of the output folder --inputs INPUTS [INPUTS ...], -i INPUTS [INPUTS ...] input csv files","title":"ETL Quick Start "},{"location":"CoConnectTools/source_code/#setup","text":"To run this example, obtain the location of the coconnect data folder, and set this as an environment variable for ease. export COCONNECT_DATA_FOLDER=$(coconnect info data_folder)","title":"Setup"},{"location":"CoConnectTools/source_code/#execute","text":"The example dataset and associated mapping rules can be run with the simple script etlcdm.py : etlcdm.py -i $COCONNECT_DATA_FOLDER /test/inputs/*.csv --rules $COCONNECT_DATA_FOLDER /test/rules/rules_14June2021.json -o test/","title":"Execute"},{"location":"CoConnectTools/source_code/#inspecting-the-output","text":".csv files are created for each CDM table, e.g. person.csv . Additionally logs are created and stored in the sub-folder logs/ . $ tree test/ test/ \u251c\u2500\u2500 condition_occurrence.csv \u251c\u2500\u2500 logs \u2502 \u2514\u2500\u2500 2021-06-16T100657.json \u251c\u2500\u2500 observation.csv \u2514\u2500\u2500 person.csv A convenience command exists to be able to display the output dataframe to the command-line: $ coconnect display dataframe test/person.csv person_id gender_concept_id ... ethnicity_source_value ethnicity_source_concept_id 0 101 8507 ... NaN NaN 1 102 8507 ... NaN NaN 2 103 8532 ... NaN NaN 3 104 8532 ... NaN NaN 4 105 8532 ... NaN NaN 5 106 8507 ... NaN NaN 6 107 8532 ... NaN NaN 7 108 8507 ... NaN NaN 8 109 8532 ... NaN NaN 9 110 8532 ... NaN NaN [10 rows x 18 columns] This can also be used with the option --drop-na to just display those columns which have none-NaN values $ coconnect display dataframe --drop-na test/person.csv person_id gender_concept_id birth_datetime gender_source_value gender_source_concept_id 0 101 8507 1951-12-25 00:00:00 M 8507 1 102 8507 1981-11-19 00:00:00 M 8507 2 103 8532 1997-05-11 00:00:00 F 8532 3 104 8532 1975-06-07 00:00:00 F 8532 4 105 8532 1976-04-23 00:00:00 F 8532 5 106 8507 1966-09-29 00:00:00 M 8507 6 107 8532 1956-11-12 00:00:00 F 8532 7 108 8507 1985-03-01 00:00:00 M 8507 8 109 8532 1950-10-31 00:00:00 F 8532 9 110 8532 1993-09-07 00:00:00 F 8532 Markdown format can be obtained for convenience: $ coconnect display dataframe --markdown --drop-na test/person.csv person_id gender_concept_id birth_datetime gender_source_value gender_source_concept_id 0 101 8507 1951-12-25 00:00:00 M 8507 1 102 8507 1981-11-19 00:00:00 M 8507 2 103 8532 1997-05-11 00:00:00 F 8532 3 104 8532 1975-06-07 00:00:00 F 8532 4 105 8532 1976-04-23 00:00:00 F 8532 5 106 8507 1966-09-29 00:00:00 M 8507 6 107 8532 1956-11-12 00:00:00 F 8532 7 108 8507 1985-03-01 00:00:00 M 8507 8 109 8532 1950-10-31 00:00:00 F 8532 9 110 8532 1993-09-07 00:00:00 F 8532","title":"Inspecting the Output"},{"location":"CoConnectTools/source_code/#command-line-interface","text":"To list the available commands available with the cli , you can simply use --help : $ coconnect --help Usage: coconnect [ OPTIONS ] COMMAND [ ARGS ] ... Options: -l, --loglevel TEXT --help Show this message and exit. Commands: map","title":"Command Line Interface "},{"location":"CoConnectTools/source_code/#map","text":"Commands used for mapping datasets with OMOP are found with the command map $ coconnect map --help Usage: coconnect map [ OPTIONS ] COMMAND [ ARGS ] ... Options: --help Show this message and exit. Commands: display Display the OMOP mapping json as a DAG list List all the python classes there are available to run make Generate a python class from the OMOP mapping json run Perform OMOP Mapping show Show the OMOP mapping json","title":"Map"},{"location":"CoConnectTools/source_code/#example","text":"This quick example shows how you can run OMOP mapping based on a sample json file Firstly run show to display the input json structure $ coconnect map show example/sample_config/lion_structural_mapping.json ... \"cdm\": { \"person\": [ { \"birth_datetime\": { \"source_table\": \"demo.csv\", \"source_field\": \"dob\", \"term_mapping\": null, \"operations\": [ \"get_datetime\" ] }, ... To display this json as a dag you can run: $ coconnect map display example/sample_config/lion_structural_mapping.json The next step is to create a .py configuration file from this input json . The tool automatically registers these files, to see registered files, you can run: $ coconnect map list {} Showing that no files have been created and registered. To create your first configuration file, run make specifying the name of the output file/class: $ coconnect map make --name Lion example/sample_config/lion_structural_mapping.json Making new file <your install dir>/co-connect-tools/coconnect/cdm/classes/Lion.py Looking at the file that has been created, you can see the .py configuration file has been made: from coconnect.cdm import define_person , define_condition_occurrence , define_visit_occurrence , define_measurement , define_observation from coconnect.cdm import CommonDataModel import json class Lion ( CommonDataModel ): ... @define_person def person_0 ( self ): \"\"\" Create CDM object for person \"\"\" self . birth_datetime = self . inputs [ \"demo.csv\" ][ \"dob\" ] self . day_of_birth = self . inputs [ \"demo.csv\" ][ \"dob\" ] self . gender_concept_id = self . inputs [ \"demo.csv\" ][ \"gender\" ] self . gender_source_concept_id = self . inputs [ \"demo.csv\" ][ \"gender\" ] self . gender_source_value = self . inputs [ \"demo.csv\" ][ \"gender\" ] self . month_of_birth = self . inputs [ \"demo.csv\" ][ \"dob\" ] .... Now the list command shows that the file has been registered with the tool: $ coconnect map list { \"Lion\": { \"module\": \"coconnect.cdm.classes.Lion\", \"path\": \"/Users/calummacdonald/Usher/CO-CONNECT/Software/co-connect-tools/coconnect/cdm/classes/Lion.py\", \"last-modified\": \"2021-04-16 10:26:33\" } } Now we're able to run: $ coconnect map run --name Lion example/sample_input_data/*.csv ... 2021-04-16 10:29:12 - Lion - INFO - finalised observation 2021-04-16 10:29:12 - Lion - INFO - saving person to output_data//person.csv 2021-04-16 10:29:12 - Lion - INFO - gender_concept_id year_of_birth ... race_source_concept_id ethnicity_source_value person_id ... 1 8532 1962 ... 123456 2 8507 1972 ... 123456 3 8532 1979 ... 123422 4 8532 1991 ... 123456 [4 rows x 12 columns] ... Outputs are saved in the folder output_data","title":"Example"},{"location":"CoConnectTools/source_code/#all-in-one","text":"In one command, all the above steps can be executed as such: Example: coconnect map run --name Lion --rules example/sample_config/lion_structural_mapping.json example/sample_input_data/*.csv To run in one command, supply the name of the dataset (e.g. Panther), the rules json file that has been obtained from mapping-pipeline and then all the input files to run on. coconnect map run --name <NAME> --rules <RULES.json> <INPUTFILE 1 > <INPUTFILE 2 > ....","title":"All in One"},{"location":"CoConnectTools/source_code/LICENSE/","text":"MIT License Copyright \u00a9 year Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"LICENSE"},{"location":"CoConnectTools/source_code/notebooks/Introduction%20-%20Using%20the%20ETLTool/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Installing \u00b6 The best way is to install the module via pip . ! pip3 install co - connect - tools - q Loading the Rules \u00b6 import coconnect.tools import json rules = coconnect . tools . load_json ( 'test/rules/rules_14June2021.json' ) print ( json . dumps ( rules , indent = 2 )[ 0 : 500 ]) { \"metadata\": { \"date_created\": \"2021-06-14T15:27:37.123947\", \"dataset\": \"Test\" }, \"cdm\": { \"observation\": [ { \"observation_concept_id\": { \"source_table\": \"Demographics.csv\", \"source_field\": \"ethnicity\", \"term_mapping\": { \"Asian\": 35825508 } }, \"observation_datetime\": { \"source_table\": \"Demographics.csv\", \"source_field\": \"date_of_birth\" }, \"observation_source_co Loading the input data \u00b6 A convienience function is available to create a map between a file name and a file path for all files in a directory: f_map = coconnect . tools . get_file_map_from_dir ( 'test/inputs/' ) f_map {'Symptoms.csv': '/Users/calummacdonald/Usher/CO-CONNECT/Software/clean/co-connect-tools/coconnect/data/test/inputs/Symptoms.csv', 'Covid19_test.csv': '/Users/calummacdonald/Usher/CO-CONNECT/Software/clean/co-connect-tools/coconnect/data/test/inputs/Covid19_test.csv', 'covid19_antibody.csv': '/Users/calummacdonald/Usher/CO-CONNECT/Software/clean/co-connect-tools/coconnect/data/test/inputs/covid19_antibody.csv', 'vaccine.csv': '/Users/calummacdonald/Usher/CO-CONNECT/Software/clean/co-connect-tools/coconnect/data/test/inputs/vaccine.csv', 'Demographics.csv': '/Users/calummacdonald/Usher/CO-CONNECT/Software/clean/co-connect-tools/coconnect/data/test/inputs/Demographics.csv'} use the f_map to load all the inputs into a map between the file name and a dataframe object inputs = coconnect . tools . load_csv ( f_map ) Creating a CDM \u00b6 from coconnect.cdm import CommonDataModel cdm = CommonDataModel ( name = rules [ 'metadata' ][ 'dataset' ], inputs = inputs , output_folder = 'output_dir/' ) cdm 2021-06-15 16:19:01 - CommonDataModel - INFO - CommonDataModel created <coconnect.cdm.model.CommonDataModel at 0x120b52400> Adding CDM Objects to the CDM \u00b6 Loop over all the rules, creating and adding a new CDM object (e.g. Person) to the CDM from coconnect.cdm import get_cdm_class from coconnect.tools import apply_rules for destination_table , rules_set in rules [ 'cdm' ] . items (): for i , rules in enumerate ( rules_set ): obj = get_cdm_class ( destination_table )() obj . set_name ( f \" { destination_table } _ { i } \" ) apply_rules ( cdm , obj , rules ) cdm . add ( obj ) 2021-06-15 16:19:01 - CommonDataModel - INFO - Added observation_0 of type observation 2021-06-15 16:19:01 - CommonDataModel - INFO - Added observation_1 of type observation 2021-06-15 16:19:01 - CommonDataModel - INFO - Added observation_2 of type observation 2021-06-15 16:19:01 - CommonDataModel - INFO - Added observation_3 of type observation 2021-06-15 16:19:01 - CommonDataModel - INFO - Added observation_4 of type observation 2021-06-15 16:19:01 - CommonDataModel - INFO - Added observation_5 of type observation 2021-06-15 16:19:01 - CommonDataModel - INFO - Added condition_occurrence_0 of type condition_occurrence 2021-06-15 16:19:01 - CommonDataModel - INFO - Added person_0 of type person 2021-06-15 16:19:01 - CommonDataModel - INFO - Added person_1 of type person see what objects we have created.. cdm . objects () {'observation': {'observation_0': <coconnect.cdm.objects.observation.Observation at 0x120ba04f0>, 'observation_1': <coconnect.cdm.objects.observation.Observation at 0x107fe61c0>, 'observation_2': <coconnect.cdm.objects.observation.Observation at 0x120ba8610>, 'observation_3': <coconnect.cdm.objects.observation.Observation at 0x120ba8550>, 'observation_4': <coconnect.cdm.objects.observation.Observation at 0x120b52ca0>, 'observation_5': <coconnect.cdm.objects.observation.Observation at 0x120badfd0>}, 'condition_occurrence': {'condition_occurrence_0': <coconnect.cdm.objects.condition_occurrence.ConditionOccurrence at 0x120badeb0>}, 'person': {'person_0': <coconnect.cdm.objects.person.Person at 0x120badf40>, 'person_1': <coconnect.cdm.objects.person.Person at 0x120bb2cd0>}} Process The CDM \u00b6 cdm . process () 2021-06-15 16:19:03 - CommonDataModel - INFO - Starting processing in order: ['person', 'observation', 'condition_occurrence'] 2021-06-15 16:19:03 - CommonDataModel - INFO - Number of objects to process for each table... { \"observation\": 6, \"condition_occurrence\": 1, \"person\": 2 } 2021-06-15 16:19:03 - CommonDataModel - INFO - for person: found 2 objects 2021-06-15 16:19:03 - CommonDataModel - INFO - working on person 2021-06-15 16:19:03 - person_0 - WARNING - Requiring non-null values in gender_concept_id removed 4 rows, leaving 6 rows. 2021-06-15 16:19:03 - CommonDataModel - INFO - finished person_0 ... 0/2, 6 rows 2021-06-15 16:19:03 - person_1 - WARNING - Requiring non-null values in gender_concept_id removed 6 rows, leaving 4 rows. 2021-06-15 16:19:03 - CommonDataModel - INFO - finished person_1 ... 1/2, 4 rows 2021-06-15 16:19:03 - CommonDataModel - INFO - Merging 2 objects for person 2021-06-15 16:19:03 - CommonDataModel - INFO - finalised person 2021-06-15 16:19:03 - CommonDataModel - INFO - for observation: found 6 objects 2021-06-15 16:19:03 - CommonDataModel - INFO - working on observation 2021-06-15 16:19:03 - observation_0 - WARNING - Requiring non-null values in observation_concept_id removed 9 rows, leaving 1 rows. 2021-06-15 16:19:03 - CommonDataModel - INFO - finished observation_0 ... 0/6, 1 rows 2021-06-15 16:19:03 - observation_1 - WARNING - Requiring non-null values in observation_concept_id removed 9 rows, leaving 1 rows. 2021-06-15 16:19:03 - CommonDataModel - INFO - finished observation_1 ... 1/6, 1 rows 2021-06-15 16:19:03 - observation_2 - WARNING - Requiring non-null values in observation_concept_id removed 9 rows, leaving 1 rows. 2021-06-15 16:19:03 - CommonDataModel - INFO - finished observation_2 ... 2/6, 1 rows 2021-06-15 16:19:03 - observation_3 - WARNING - Requiring non-null values in observation_concept_id removed 7 rows, leaving 3 rows. 2021-06-15 16:19:03 - CommonDataModel - INFO - finished observation_3 ... 3/6, 3 rows 2021-06-15 16:19:03 - observation_4 - WARNING - Requiring non-null values in observation_concept_id removed 8 rows, leaving 2 rows. 2021-06-15 16:19:03 - CommonDataModel - INFO - finished observation_4 ... 4/6, 2 rows 2021-06-15 16:19:03 - observation_5 - WARNING - Requiring non-null values in observation_concept_id removed 9 rows, leaving 1 rows. 2021-06-15 16:19:03 - CommonDataModel - INFO - finished observation_5 ... 5/6, 1 rows 2021-06-15 16:19:03 - CommonDataModel - INFO - Merging 6 objects for observation 2021-06-15 16:19:03 - CommonDataModel - INFO - finalised observation 2021-06-15 16:19:03 - CommonDataModel - INFO - for condition_occurrence: found 1 object 2021-06-15 16:19:03 - CommonDataModel - INFO - working on condition_occurrence 2021-06-15 16:19:03 - condition_occurrence_0 - WARNING - Requiring non-null values in person_id removed 14 rows, leaving 8 rows. 2021-06-15 16:19:03 - condition_occurrence_0 - WARNING - Requiring non-null values in condition_concept_id removed 4 rows, leaving 4 rows. 2021-06-15 16:19:04 - CommonDataModel - INFO - finished condition_occurrence_0 ... 0/1, 4 rows 2021-06-15 16:19:04 - CommonDataModel - INFO - Merging 1 objects for condition_occurrence 2021-06-15 16:19:04 - CommonDataModel - INFO - finalised condition_occurrence 2021-06-15 16:19:04 - CommonDataModel - INFO - saving person to output_dir//person.csv 2021-06-15 16:19:04 - CommonDataModel - INFO - gender_concept_id birth_datetime person_source_value \\ person_id 101 8507 1951-12-25 00:00:00 102 8507 1981-11-19 00:00:00 103 8532 1997-05-11 00:00:00 104 8532 1975-06-07 00:00:00 105 8532 1976-04-23 00:00:00 106 8507 1966-09-29 00:00:00 107 8532 1956-11-12 00:00:00 108 8507 1985-03-01 00:00:00 109 8532 1950-10-31 00:00:00 110 8532 1993-09-07 00:00:00 gender_source_value gender_source_concept_id race_source_value \\ person_id 101 M 8507 102 M 8507 103 F 8532 104 F 8532 105 F 8532 106 M 8507 107 F 8532 108 M 8507 109 F 8532 110 F 8532 ethnicity_source_value person_id 101 102 103 104 105 106 107 108 109 110 2021-06-15 16:19:04 - CommonDataModel - INFO - saving observation to output_dir//observation.csv 2021-06-15 16:19:04 - CommonDataModel - INFO - person_id observation_concept_id observation_datetime \\ observation_id 1 107 35825508 1956-11-12 00:00:00 2 104 35825531 1975-06-07 00:00:00 3 103 35826241 1997-05-11 00:00:00 4 101 35827394 1951-12-25 00:00:00 5 105 35827394 1976-04-23 00:00:00 6 110 35827394 1993-09-07 00:00:00 7 102 35825567 1981-11-19 00:00:00 8 106 35825567 1966-09-29 00:00:00 9 108 35827395 1985-03-01 00:00:00 value_as_string observation_source_value \\ observation_id 1 Asian 2 Bangladeshi 3 Indian 4 White 5 White 6 White 7 Black 8 Black 9 White and Asian observation_source_concept_id unit_source_value \\ observation_id 1 35825508 2 35825531 3 35826241 4 35827394 5 35827394 6 35827394 7 35825567 8 35825567 9 35827395 qualifier_source_value observation_id 1 2 3 4 5 6 7 8 9 2021-06-15 16:19:04 - CommonDataModel - INFO - saving condition_occurrence to output_dir//condition_occurrence.csv 2021-06-15 16:19:04 - CommonDataModel - INFO - person_id condition_concept_id \\ condition_occurrence_id 1 101 254761 2 102 254761 3 103 254761 4 105 254761 condition_start_datetime condition_end_datetime \\ condition_occurrence_id 1 2020-11-15 00:00:00 2020-11-15 00:00:00 2 2020-01-04 00:00:00 2020-01-04 00:00:00 3 2020-03-27 00:00:00 2020-03-27 00:00:00 4 2020-07-27 00:00:00 2020-07-27 00:00:00 stop_reason condition_source_value \\ condition_occurrence_id 1 Y 2 Y 3 Y 4 Y condition_source_concept_id \\ condition_occurrence_id 1 254761 2 254761 3 254761 4 254761 condition_status_source_value condition_occurrence_id 1 2 3 4 2021-06-15 16:19:04 - CommonDataModel - INFO - saved a log file to output_dir//logs//2021-06-15T151901.json Inspect Outputs \u00b6 cdm . keys () dict_keys(['person', 'observation', 'condition_occurrence']) cdm [ 'person' ] . dropna ( axis = 1 , how = 'all' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } gender_concept_id birth_datetime person_source_value gender_source_value gender_source_concept_id race_source_value ethnicity_source_value person_id 101 8507 1951-12-25 00:00:00 M 8507 102 8507 1981-11-19 00:00:00 M 8507 103 8532 1997-05-11 00:00:00 F 8532 104 8532 1975-06-07 00:00:00 F 8532 105 8532 1976-04-23 00:00:00 F 8532 106 8507 1966-09-29 00:00:00 M 8507 107 8532 1956-11-12 00:00:00 F 8532 108 8507 1985-03-01 00:00:00 M 8507 109 8532 1950-10-31 00:00:00 F 8532 110 8532 1993-09-07 00:00:00 F 8532 cdm [ 'observation' ] . dropna ( axis = 1 , how = 'all' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } person_id observation_concept_id observation_datetime value_as_string observation_source_value observation_source_concept_id unit_source_value qualifier_source_value observation_id 1 107 35825508 1956-11-12 00:00:00 Asian 35825508 2 104 35825531 1975-06-07 00:00:00 Bangladeshi 35825531 3 103 35826241 1997-05-11 00:00:00 Indian 35826241 4 101 35827394 1951-12-25 00:00:00 White 35827394 5 105 35827394 1976-04-23 00:00:00 White 35827394 6 110 35827394 1993-09-07 00:00:00 White 35827394 7 102 35825567 1981-11-19 00:00:00 Black 35825567 8 106 35825567 1966-09-29 00:00:00 Black 35825567 9 108 35827395 1985-03-01 00:00:00 White and Asian 35827395 cdm [ 'condition_occurrence' ] . dropna ( axis = 1 , how = 'all' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } person_id condition_concept_id condition_start_datetime condition_end_datetime stop_reason condition_source_value condition_source_concept_id condition_status_source_value condition_occurrence_id 1 101 254761 2020-11-15 00:00:00 2020-11-15 00:00:00 Y 254761 2 102 254761 2020-01-04 00:00:00 2020-01-04 00:00:00 Y 254761 3 103 254761 2020-03-27 00:00:00 2020-03-27 00:00:00 Y 254761 4 105 254761 2020-07-27 00:00:00 2020-07-27 00:00:00 Y 254761","title":"Workbook Example"},{"location":"CoConnectTools/source_code/notebooks/Introduction%20-%20Using%20the%20ETLTool/#installing","text":"The best way is to install the module via pip . ! pip3 install co - connect - tools - q","title":"Installing"},{"location":"CoConnectTools/source_code/notebooks/Introduction%20-%20Using%20the%20ETLTool/#loading-the-rules","text":"import coconnect.tools import json rules = coconnect . tools . load_json ( 'test/rules/rules_14June2021.json' ) print ( json . dumps ( rules , indent = 2 )[ 0 : 500 ]) { \"metadata\": { \"date_created\": \"2021-06-14T15:27:37.123947\", \"dataset\": \"Test\" }, \"cdm\": { \"observation\": [ { \"observation_concept_id\": { \"source_table\": \"Demographics.csv\", \"source_field\": \"ethnicity\", \"term_mapping\": { \"Asian\": 35825508 } }, \"observation_datetime\": { \"source_table\": \"Demographics.csv\", \"source_field\": \"date_of_birth\" }, \"observation_source_co","title":"Loading the Rules"},{"location":"CoConnectTools/source_code/notebooks/Introduction%20-%20Using%20the%20ETLTool/#loading-the-input-data","text":"A convienience function is available to create a map between a file name and a file path for all files in a directory: f_map = coconnect . tools . get_file_map_from_dir ( 'test/inputs/' ) f_map {'Symptoms.csv': '/Users/calummacdonald/Usher/CO-CONNECT/Software/clean/co-connect-tools/coconnect/data/test/inputs/Symptoms.csv', 'Covid19_test.csv': '/Users/calummacdonald/Usher/CO-CONNECT/Software/clean/co-connect-tools/coconnect/data/test/inputs/Covid19_test.csv', 'covid19_antibody.csv': '/Users/calummacdonald/Usher/CO-CONNECT/Software/clean/co-connect-tools/coconnect/data/test/inputs/covid19_antibody.csv', 'vaccine.csv': '/Users/calummacdonald/Usher/CO-CONNECT/Software/clean/co-connect-tools/coconnect/data/test/inputs/vaccine.csv', 'Demographics.csv': '/Users/calummacdonald/Usher/CO-CONNECT/Software/clean/co-connect-tools/coconnect/data/test/inputs/Demographics.csv'} use the f_map to load all the inputs into a map between the file name and a dataframe object inputs = coconnect . tools . load_csv ( f_map )","title":"Loading the input data"},{"location":"CoConnectTools/source_code/notebooks/Introduction%20-%20Using%20the%20ETLTool/#creating-a-cdm","text":"from coconnect.cdm import CommonDataModel cdm = CommonDataModel ( name = rules [ 'metadata' ][ 'dataset' ], inputs = inputs , output_folder = 'output_dir/' ) cdm 2021-06-15 16:19:01 - CommonDataModel - INFO - CommonDataModel created <coconnect.cdm.model.CommonDataModel at 0x120b52400>","title":"Creating a CDM"},{"location":"CoConnectTools/source_code/notebooks/Introduction%20-%20Using%20the%20ETLTool/#adding-cdm-objects-to-the-cdm","text":"Loop over all the rules, creating and adding a new CDM object (e.g. Person) to the CDM from coconnect.cdm import get_cdm_class from coconnect.tools import apply_rules for destination_table , rules_set in rules [ 'cdm' ] . items (): for i , rules in enumerate ( rules_set ): obj = get_cdm_class ( destination_table )() obj . set_name ( f \" { destination_table } _ { i } \" ) apply_rules ( cdm , obj , rules ) cdm . add ( obj ) 2021-06-15 16:19:01 - CommonDataModel - INFO - Added observation_0 of type observation 2021-06-15 16:19:01 - CommonDataModel - INFO - Added observation_1 of type observation 2021-06-15 16:19:01 - CommonDataModel - INFO - Added observation_2 of type observation 2021-06-15 16:19:01 - CommonDataModel - INFO - Added observation_3 of type observation 2021-06-15 16:19:01 - CommonDataModel - INFO - Added observation_4 of type observation 2021-06-15 16:19:01 - CommonDataModel - INFO - Added observation_5 of type observation 2021-06-15 16:19:01 - CommonDataModel - INFO - Added condition_occurrence_0 of type condition_occurrence 2021-06-15 16:19:01 - CommonDataModel - INFO - Added person_0 of type person 2021-06-15 16:19:01 - CommonDataModel - INFO - Added person_1 of type person see what objects we have created.. cdm . objects () {'observation': {'observation_0': <coconnect.cdm.objects.observation.Observation at 0x120ba04f0>, 'observation_1': <coconnect.cdm.objects.observation.Observation at 0x107fe61c0>, 'observation_2': <coconnect.cdm.objects.observation.Observation at 0x120ba8610>, 'observation_3': <coconnect.cdm.objects.observation.Observation at 0x120ba8550>, 'observation_4': <coconnect.cdm.objects.observation.Observation at 0x120b52ca0>, 'observation_5': <coconnect.cdm.objects.observation.Observation at 0x120badfd0>}, 'condition_occurrence': {'condition_occurrence_0': <coconnect.cdm.objects.condition_occurrence.ConditionOccurrence at 0x120badeb0>}, 'person': {'person_0': <coconnect.cdm.objects.person.Person at 0x120badf40>, 'person_1': <coconnect.cdm.objects.person.Person at 0x120bb2cd0>}}","title":"Adding CDM Objects to the CDM"},{"location":"CoConnectTools/source_code/notebooks/Introduction%20-%20Using%20the%20ETLTool/#process-the-cdm","text":"cdm . process () 2021-06-15 16:19:03 - CommonDataModel - INFO - Starting processing in order: ['person', 'observation', 'condition_occurrence'] 2021-06-15 16:19:03 - CommonDataModel - INFO - Number of objects to process for each table... { \"observation\": 6, \"condition_occurrence\": 1, \"person\": 2 } 2021-06-15 16:19:03 - CommonDataModel - INFO - for person: found 2 objects 2021-06-15 16:19:03 - CommonDataModel - INFO - working on person 2021-06-15 16:19:03 - person_0 - WARNING - Requiring non-null values in gender_concept_id removed 4 rows, leaving 6 rows. 2021-06-15 16:19:03 - CommonDataModel - INFO - finished person_0 ... 0/2, 6 rows 2021-06-15 16:19:03 - person_1 - WARNING - Requiring non-null values in gender_concept_id removed 6 rows, leaving 4 rows. 2021-06-15 16:19:03 - CommonDataModel - INFO - finished person_1 ... 1/2, 4 rows 2021-06-15 16:19:03 - CommonDataModel - INFO - Merging 2 objects for person 2021-06-15 16:19:03 - CommonDataModel - INFO - finalised person 2021-06-15 16:19:03 - CommonDataModel - INFO - for observation: found 6 objects 2021-06-15 16:19:03 - CommonDataModel - INFO - working on observation 2021-06-15 16:19:03 - observation_0 - WARNING - Requiring non-null values in observation_concept_id removed 9 rows, leaving 1 rows. 2021-06-15 16:19:03 - CommonDataModel - INFO - finished observation_0 ... 0/6, 1 rows 2021-06-15 16:19:03 - observation_1 - WARNING - Requiring non-null values in observation_concept_id removed 9 rows, leaving 1 rows. 2021-06-15 16:19:03 - CommonDataModel - INFO - finished observation_1 ... 1/6, 1 rows 2021-06-15 16:19:03 - observation_2 - WARNING - Requiring non-null values in observation_concept_id removed 9 rows, leaving 1 rows. 2021-06-15 16:19:03 - CommonDataModel - INFO - finished observation_2 ... 2/6, 1 rows 2021-06-15 16:19:03 - observation_3 - WARNING - Requiring non-null values in observation_concept_id removed 7 rows, leaving 3 rows. 2021-06-15 16:19:03 - CommonDataModel - INFO - finished observation_3 ... 3/6, 3 rows 2021-06-15 16:19:03 - observation_4 - WARNING - Requiring non-null values in observation_concept_id removed 8 rows, leaving 2 rows. 2021-06-15 16:19:03 - CommonDataModel - INFO - finished observation_4 ... 4/6, 2 rows 2021-06-15 16:19:03 - observation_5 - WARNING - Requiring non-null values in observation_concept_id removed 9 rows, leaving 1 rows. 2021-06-15 16:19:03 - CommonDataModel - INFO - finished observation_5 ... 5/6, 1 rows 2021-06-15 16:19:03 - CommonDataModel - INFO - Merging 6 objects for observation 2021-06-15 16:19:03 - CommonDataModel - INFO - finalised observation 2021-06-15 16:19:03 - CommonDataModel - INFO - for condition_occurrence: found 1 object 2021-06-15 16:19:03 - CommonDataModel - INFO - working on condition_occurrence 2021-06-15 16:19:03 - condition_occurrence_0 - WARNING - Requiring non-null values in person_id removed 14 rows, leaving 8 rows. 2021-06-15 16:19:03 - condition_occurrence_0 - WARNING - Requiring non-null values in condition_concept_id removed 4 rows, leaving 4 rows. 2021-06-15 16:19:04 - CommonDataModel - INFO - finished condition_occurrence_0 ... 0/1, 4 rows 2021-06-15 16:19:04 - CommonDataModel - INFO - Merging 1 objects for condition_occurrence 2021-06-15 16:19:04 - CommonDataModel - INFO - finalised condition_occurrence 2021-06-15 16:19:04 - CommonDataModel - INFO - saving person to output_dir//person.csv 2021-06-15 16:19:04 - CommonDataModel - INFO - gender_concept_id birth_datetime person_source_value \\ person_id 101 8507 1951-12-25 00:00:00 102 8507 1981-11-19 00:00:00 103 8532 1997-05-11 00:00:00 104 8532 1975-06-07 00:00:00 105 8532 1976-04-23 00:00:00 106 8507 1966-09-29 00:00:00 107 8532 1956-11-12 00:00:00 108 8507 1985-03-01 00:00:00 109 8532 1950-10-31 00:00:00 110 8532 1993-09-07 00:00:00 gender_source_value gender_source_concept_id race_source_value \\ person_id 101 M 8507 102 M 8507 103 F 8532 104 F 8532 105 F 8532 106 M 8507 107 F 8532 108 M 8507 109 F 8532 110 F 8532 ethnicity_source_value person_id 101 102 103 104 105 106 107 108 109 110 2021-06-15 16:19:04 - CommonDataModel - INFO - saving observation to output_dir//observation.csv 2021-06-15 16:19:04 - CommonDataModel - INFO - person_id observation_concept_id observation_datetime \\ observation_id 1 107 35825508 1956-11-12 00:00:00 2 104 35825531 1975-06-07 00:00:00 3 103 35826241 1997-05-11 00:00:00 4 101 35827394 1951-12-25 00:00:00 5 105 35827394 1976-04-23 00:00:00 6 110 35827394 1993-09-07 00:00:00 7 102 35825567 1981-11-19 00:00:00 8 106 35825567 1966-09-29 00:00:00 9 108 35827395 1985-03-01 00:00:00 value_as_string observation_source_value \\ observation_id 1 Asian 2 Bangladeshi 3 Indian 4 White 5 White 6 White 7 Black 8 Black 9 White and Asian observation_source_concept_id unit_source_value \\ observation_id 1 35825508 2 35825531 3 35826241 4 35827394 5 35827394 6 35827394 7 35825567 8 35825567 9 35827395 qualifier_source_value observation_id 1 2 3 4 5 6 7 8 9 2021-06-15 16:19:04 - CommonDataModel - INFO - saving condition_occurrence to output_dir//condition_occurrence.csv 2021-06-15 16:19:04 - CommonDataModel - INFO - person_id condition_concept_id \\ condition_occurrence_id 1 101 254761 2 102 254761 3 103 254761 4 105 254761 condition_start_datetime condition_end_datetime \\ condition_occurrence_id 1 2020-11-15 00:00:00 2020-11-15 00:00:00 2 2020-01-04 00:00:00 2020-01-04 00:00:00 3 2020-03-27 00:00:00 2020-03-27 00:00:00 4 2020-07-27 00:00:00 2020-07-27 00:00:00 stop_reason condition_source_value \\ condition_occurrence_id 1 Y 2 Y 3 Y 4 Y condition_source_concept_id \\ condition_occurrence_id 1 254761 2 254761 3 254761 4 254761 condition_status_source_value condition_occurrence_id 1 2 3 4 2021-06-15 16:19:04 - CommonDataModel - INFO - saved a log file to output_dir//logs//2021-06-15T151901.json","title":"Process The CDM"},{"location":"CoConnectTools/source_code/notebooks/Introduction%20-%20Using%20the%20ETLTool/#inspect-outputs","text":"cdm . keys () dict_keys(['person', 'observation', 'condition_occurrence']) cdm [ 'person' ] . dropna ( axis = 1 , how = 'all' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } gender_concept_id birth_datetime person_source_value gender_source_value gender_source_concept_id race_source_value ethnicity_source_value person_id 101 8507 1951-12-25 00:00:00 M 8507 102 8507 1981-11-19 00:00:00 M 8507 103 8532 1997-05-11 00:00:00 F 8532 104 8532 1975-06-07 00:00:00 F 8532 105 8532 1976-04-23 00:00:00 F 8532 106 8507 1966-09-29 00:00:00 M 8507 107 8532 1956-11-12 00:00:00 F 8532 108 8507 1985-03-01 00:00:00 M 8507 109 8532 1950-10-31 00:00:00 F 8532 110 8532 1993-09-07 00:00:00 F 8532 cdm [ 'observation' ] . dropna ( axis = 1 , how = 'all' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } person_id observation_concept_id observation_datetime value_as_string observation_source_value observation_source_concept_id unit_source_value qualifier_source_value observation_id 1 107 35825508 1956-11-12 00:00:00 Asian 35825508 2 104 35825531 1975-06-07 00:00:00 Bangladeshi 35825531 3 103 35826241 1997-05-11 00:00:00 Indian 35826241 4 101 35827394 1951-12-25 00:00:00 White 35827394 5 105 35827394 1976-04-23 00:00:00 White 35827394 6 110 35827394 1993-09-07 00:00:00 White 35827394 7 102 35825567 1981-11-19 00:00:00 Black 35825567 8 106 35825567 1966-09-29 00:00:00 Black 35825567 9 108 35827395 1985-03-01 00:00:00 White and Asian 35827395 cdm [ 'condition_occurrence' ] . dropna ( axis = 1 , how = 'all' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } person_id condition_concept_id condition_start_datetime condition_end_datetime stop_reason condition_source_value condition_source_concept_id condition_status_source_value condition_occurrence_id 1 101 254761 2020-11-15 00:00:00 2020-11-15 00:00:00 Y 254761 2 102 254761 2020-01-04 00:00:00 2020-01-04 00:00:00 Y 254761 3 103 254761 2020-03-27 00:00:00 2020-03-27 00:00:00 Y 254761 4 105 254761 2020-07-27 00:00:00 2020-07-27 00:00:00 Y 254761","title":"Inspect Outputs"},{"location":"GettingStarted/OHDSI-in-a-Box/","text":"OHDSI-in-a-box Is a virtual machine, that runs on Linux (Ubuntu), it contains OHDSI tools and sample data for personal learning and training enviroments. Instructions can be found for deploying the OHDSI-in-a-box on AWS if you like. Otherwise you can request access to asmall 4GB virtual-machine hosted on the University of Dundee SharePoint","title":"OHDSI in a Box"},{"location":"GettingStarted/db_from_ohdsi_box/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); import sqlalchemy as sql import pandas as pd ngin = sql . create_engine ( 'postgresql://etldev:etldev@192.168.0.104:5432/postgres' ) cnx = ngin . connect () inspector = sql . inspect ( ngin ) def get_df ( schema , table_name ): selection = r ''' SELECT * FROM %s . %s ''' % ( schema , table_name ) return pd . read_sql ( selection , ngin ) for schema in inspector . get_schema_names (): print ( schema ) information_schema omop_vocabulary public synthea_etl synthea_omop synthea_omop_results synthea_raw for schema in inspector . get_schema_names (): print ( f '===== { schema } ===== ' ) for table in inspector . get_table_names ( schema = schema ): print ( ' \\t ' , table ) ===== information_schema ===== sql_parts sql_languages sql_features sql_implementation_info sql_packages sql_sizing sql_sizing_profiles ===== omop_vocabulary ===== concept vocabulary domain concept_class concept_relationship relationship concept_synonym concept_ancestor drug_strength ===== public ===== ===== synthea_etl ===== source_to_standard_vocab_map ===== synthea_omop ===== condition_era measurement care_site cost dose_era drug_era fact_relationship observation payer_plan_period provider metadata source_to_concept_map visit_occurrence specimen death visit_detail cdm_source condition_occurrence device_exposure drug_exposure location note note_nlp observation_period person procedure_occurrence ===== synthea_omop_results ===== dqdashboard_results achilles_analysis achilles_results_derived achilles_heel_results achilles_results achilles_results_dist ===== synthea_raw ===== allergies careplans conditions devices encounters imaging_studies immunizations observations organizations patients payer_transitions medications payers procedures providers supplies df_patients = get_df ( 'synthea_raw' , 'patients' ) df_patients .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } id birthdate deathdate ssn drivers passport prefix first last suffix ... city state county zip lat lon healthcare_expenses healthcare_coverage load_table_id load_row_id 0 d27e78f9-55c7-40ec-9922-e05d49a4c8a5 1998-04-21 2012-10-23 999-82-2767 None None None Domingo513 Maggio310 None ... Fall River Massachusetts Bristol County 02720 41.566987 -71.047715 317032.72 2258.12 patients 1 1 9983811a-1cd8-4468-90ac-cde90f6a6b23 2008-10-17 None 999-49-2108 None None None Patti584 Hane680 None ... Melrose Massachusetts Middlesex County 02176 42.457218 -71.100029 34877.84 3335.35 patients 2 2 3b877771-2916-4e03-a1df-9b656b9f9f36 1963-07-01 1969-11-17 999-74-6598 None None None Sarina640 Metz686 None ... Warren Massachusetts Worcester County 01585 42.187558 -72.205794 5817.79 0.00 patients 3 3 4773cd3e-f603-4ef7-8088-4c75bc9b5259 1910-01-10 1975-07-21 999-26-6281 S99929918 X2326739X Mr. Eddie505 Bergnaum523 None ... Topsfield Massachusetts Essex County None 42.669755 -70.948110 1494810.11 5827.68 patients 4 4 0e517b72-2d1f-47f5-be31-ebf777539755 1988-07-12 None 999-55-8595 S99987865 X65684413X Mr. Toney527 Hane680 None ... Waltham Massachusetts Middlesex County 02472 42.418380 -71.201179 696325.58 10922.02 patients 5 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 1220 7fc69c44-b59c-4801-91ef-f378f2680b7b 1919-04-10 2008-09-18 999-58-3340 S99997252 X881694X Mrs. Pearly845 McLaughlin530 None ... Haverhill Massachusetts Essex County 01832 42.791103 -71.056032 1594169.70 204103.79 patients 1221 1221 c40558d0-31ed-4685-b518-676c66daa182 1919-04-10 2000-10-28 999-31-6022 S99979126 X47776825X Mrs. Arletha291 Ziemann98 None ... Haverhill Massachusetts Essex County None 42.795330 -71.107308 1463521.76 19584.36 patients 1222 1222 2e84e6e2-5d09-44b7-9bda-44e52e1f7b4a 1919-04-10 1990-06-24 999-39-9491 S99923441 X46487992X Mrs. Marlen929 Becker968 None ... Haverhill Massachusetts Essex County None 42.720813 -71.073197 1412238.08 11094.06 patients 1223 1223 eff80fe3-9d78-4d35-b1d5-8e99474e87d6 1934-10-29 2012-03-10 999-68-7999 S99913104 X33876402X Ms. Carmina141 Hackett68 None ... Haverhill Massachusetts Essex County 01832 42.830061 -71.100740 1595838.17 10888.12 patients 1224 1224 51c0750a-e2db-4e4f-b8d3-9e9d572dfb41 1934-10-29 None 999-97-2556 S99971181 X49554719X Mrs. Pinkie318 Heaney114 None ... Haverhill Massachusetts Essex County 01832 42.803027 -71.094824 1571421.70 24399.49 patients 1225 1225 rows \u00d7 27 columns df_patients . columns Index(['id', 'birthdate', 'deathdate', 'ssn', 'drivers', 'passport', 'prefix', 'first', 'last', 'suffix', 'maiden', 'marital', 'race', 'ethnicity', 'gender', 'birthplace', 'address', 'city', 'state', 'county', 'zip', 'lat', 'lon', 'healthcare_expenses', 'healthcare_coverage', 'load_table_id', 'load_row_id'], dtype='object') for col in df_patients . columns : series = df_patients [ col ] print ( col , series . unique () . size , series . dtype ) id 1225 object birthdate 989 object deathdate 214 object ssn 1224 object drivers 1003 object passport 952 object prefix 4 object first 1032 object last 486 object suffix 4 object maiden 258 object marital 3 object race 5 object ethnicity 2 object gender 2 object birthplace 292 object address 1225 object city 233 object state 1 object county 14 object zip 225 object lat 1225 float64 lon 1225 float64 healthcare_expenses 1225 float64 healthcare_coverage 1143 float64 load_table_id 1 object load_row_id 1225 int64 cdm = pd . read_csv ( 'CommonDataModel/OMOP_CDM_v5_3_1.csv' , sep = \",\" , encoding = 'Latin-1' ) with pd . option_context ( 'display.max_colwidth' , - 1 ): display ( cdm ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Unnamed: 0 field required type description table schema 0 1 condition_occurrence_id Yes INTEGER A unique identifier for each Condition Occurrence event. condition_occurrence cdm 1 2 person_id Yes INTEGER A foreign key identifier to the Person who is experiencing the condition. The demographic details of that Person are stored in the PERSON table. condition_occurrence cdm 2 3 condition_concept_id Yes INTEGER A foreign key that refers to a Standard Condition Concept identifier in the Standardized Vocabularies. condition_occurrence cdm 3 4 condition_start_date Yes DATE The date when the instance of the Condition is recorded. condition_occurrence cdm 4 5 condition_start_datetime No DATETIME The date and time when the instance of the Condition is recorded. condition_occurrence cdm ... ... ... ... ... ... ... ... 402 403 vocabulary_id Yes VARCHAR(20) A unique identifier for each Vocabulary, such as ICD9CM, SNOMED, Visit. vocabulary cdm 403 404 vocabulary_name Yes VARCHAR(255) The name describing the vocabulary, for example \"International Classification of Diseases, Ninth Revision, Clinical Modification, Volume 1 and 2 (NCHS)\" etc. vocabulary cdm 404 405 vocabulary_reference Yes VARCHAR(255) External reference to documentation or available download of the about the vocabulary. vocabulary cdm 405 406 vocabulary_version No VARCHAR(255) Version of the Vocabulary as indicated in the source. vocabulary cdm 406 407 vocabulary_concept_id Yes INTEGER A foreign key that refers to a standard concept identifier in the CONCEPT table for the Vocabulary the VOCABULARY record belongs to. vocabulary cdm 407 rows \u00d7 7 columns cdm . set_index ([ 'table' , 'field' ]) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Unnamed: 0 required type description schema table field condition_occurrence condition_occurrence_id 1 Yes INTEGER A unique identifier for each Condition Occurre... cdm person_id 2 Yes INTEGER A foreign key identifier to the Person who is ... cdm condition_concept_id 3 Yes INTEGER A foreign key that refers to a Standard Condit... cdm condition_start_date 4 Yes DATE The date when the instance of the Condition is... cdm condition_start_datetime 5 No DATETIME The date and time when the instance of the Con... cdm ... ... ... ... ... ... ... vocabulary vocabulary_id 403 Yes VARCHAR(20) A unique identifier for each Vocabulary, such ... cdm vocabulary_name 404 Yes VARCHAR(255) The name describing the vocabulary, for exampl... cdm vocabulary_reference 405 Yes VARCHAR(255) External reference to documentation or availab... cdm vocabulary_version 406 No VARCHAR(255) Version of the Vocabulary as indicated in the ... cdm vocabulary_concept_id 407 Yes INTEGER A foreign key that refers to a standard concep... cdm 407 rows \u00d7 5 columns","title":"Db from ohdsi box"},{"location":"GettingStarted/explore-datasets-python/","text":"(function() { function addWidgetsRenderer() { var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var scriptElement = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} scriptElement.src = widgetRendererSrc; document.body.appendChild(scriptElement); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); The virtual machine contains multiple datasets in csv files as well as stored in a postgresql server. Connecting to Postgresql \u00b6 Firstly, import the python modules we want to use.. import sqlalchemy as sql import pandas as pd import matplotlib.pyplot as plt Install python packages \u00b6 If you don't have this module installed, you can open the terminal and install via: sudo apt update sudo apt install python3-pip pip3 install sqlachemy pandas matplotlib Connect to a database \u00b6 Give the ip address of the VirtualBox, if you are running these from inside the VirtualBox there you want to just use : ip_address = 'localhost' #or 127.0.0.1 If you have exposed the VirtualBox to your local network, or even the web, then specify that IP address ip_address = '192.168.0.104' Create an engine that connects to the postgres database that runs inside the VirtualBox ngin = sql . create_engine ( f 'postgresql://etldev:etldev@ { ip_address } :5432/postgres' ) Establish a connection to it: cnx = ngin . connect () Inspecting the databases and datasets \u00b6 We can use the inspector to see what databases (schemas) are available to use inspector = sql . inspect ( ngin ) [ schema for schema in inspector . get_schema_names () ] ['information_schema', 'omop_vocabulary', 'public', 'synthea_etl', 'synthea_omop', 'synthea_omop_results', 'synthea_raw'] Picking the raw \"synthea\" data, we can see what datasets (tables) are contained within the synthea_raw database: inspector . get_table_names ( schema = 'synthea_raw' ) ['allergies', 'careplans', 'conditions', 'devices', 'encounters', 'imaging_studies', 'immunizations', 'observations', 'organizations', 'patients', 'payer_transitions', 'medications', 'payers', 'procedures', 'providers', 'supplies'] SQL to Pandas Dataframe \u00b6 Using a quick function we can load the full table into a pandas dataframe and print what is given by the first 'row' of the table. def get_df ( schema_name , table_name ): selection = r ''' SELECT * FROM %s . %s ''' % ( schema_name , table_name ) return pd . read_sql ( selection , ngin ) df_conditions = get_df ( 'synthea_raw' , 'conditions' ) df_conditions . iloc [ 0 ] start 2001-07-09 stop None patient 0e517b72-2d1f-47f5-be31-ebf777539755 encounter 6d0fcf84-4411-478f-b4e4-fba643da0e61 code 128613002 description Seizure disorder load_table_id conditions load_row_id 1 Name: 0, dtype: object Displaying a Dataframe \u00b6 By using set_index() we can organise the table a bit better to display the data for each patient. Using drop() we can remove the unnessary column load_table_id which is just conditions for all data entries. df_conditions = df_conditions . set_index ([ 'patient' , 'encounter' , 'code' ]) . drop ( 'load_table_id' , axis = 1 ) df_conditions .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } start stop description load_row_id patient encounter code 0e517b72-2d1f-47f5-be31-ebf777539755 6d0fcf84-4411-478f-b4e4-fba643da0e61 128613002 2001-07-09 None Seizure disorder 1 703151001 2001-07-09 None History of single seizure (situation) 2 9156490c-ccef-40a0-bba7-a27b4691e333 84757009 2001-09-10 None Epilepsy 3 e670e3f9-067f-43da-8444-c4d236b002ce 40055000 2006-04-02 None Chronic sinusitis (disorder) 4 4773cd3e-f603-4ef7-8088-4c75bc9b5259 def28c54-0b05-4171-99b3-2da43460d7b3 162864005 1947-03-31 None Body mass index 30+ - obesity (finding) 5 ... ... ... ... ... ... ... 51c0750a-e2db-4e4f-b8d3-9e9d572dfb41 bc44e5e2-6ae0-4e17-9074-838f5c39c3ab 68962001 2020-02-28 2020-03-29 Muscle pain (finding) 15441 57676002 2020-02-28 2020-03-29 Joint pain (finding) 15442 36955009 2020-02-28 2020-03-29 Loss of taste (finding) 15443 840544004 2020-02-28 2020-02-28 Suspected COVID-19 15444 840539006 2020-02-28 2020-03-29 COVID-19 15445 15445 rows \u00d7 4 columns Inspecting a Series \u00b6 We can look at the column (or series) for description and count the number of unique values (number of times this was a registered condition). Using head(10) we can display the top 10 ranked conditions that appear. df_conditions [ 'description' ] . value_counts () . head ( 10 ) Viral sinusitis (disorder) 1284 Suspected COVID-19 911 COVID-19 881 Fever (finding) 804 Acute viral pharyngitis (disorder) 709 Cough (finding) 622 Acute bronchitis (disorder) 580 Normal pregnancy 516 Body mass index 30+ - obesity (finding) 489 Loss of taste (finding) 440 Name: description, dtype: int64 Filtering a DataFrame \u00b6 Next, we use filter the dataframe into just two columns: the start (date) and description of the condition. We also call two additional functions: 1. pd.to_datetime() helps by converting the date from a string into an datetime object 2. sort_values() helps by ordering the column based on the date df_small = df_conditions . reset_index ()[[ 'start' , 'description' ]] df_small [ 'start' ] = pd . to_datetime ( df_small [ 'start' ]) df_small = df_small . sort_values ( 'start' ) df_small We are able to filter the to an even smaller dataframe by requiring the date of start to be from 2019 onwards df_small_from_2019 = df_small [ df_small [ 'start' ] > '2019' ] df_small_from_2019 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } start description 13354 2019-01-02 Fracture of ankle 2039 2019-01-03 Acute bronchitis (disorder) 5072 2019-01-03 Viral sinusitis (disorder) 11537 2019-01-04 Acute bronchitis (disorder) 1929 2019-01-04 Cardiac Arrest ... ... ... 7351 2020-06-11 Viral sinusitis (disorder) 5037 2020-06-11 Neuropathy due to type 2 diabetes mellitus (di... 195 2020-06-14 Normal pregnancy 14300 2020-06-14 Sprain of ankle 4220 2020-06-16 Viral sinusitis (disorder) 7253 rows \u00d7 2 columns We make a series of new dataframes by filtering based on the description df_viral = df_small_from_2019 [ df_small_from_2019 [ 'description' ] . str . contains ( 'Viral' )] df_pregnancy = df_small_from_2019 [ df_small_from_2019 [ 'description' ] . str . contains ( 'pregnancy' )] len ( df_viral ), len ( df_pregnancy ) (180, 66) Further splitting up a dataframe for COVID-19 cases into confirmed and suspected cases df_covid = df_small_from_2019 [ df_small_from_2019 [ 'description' ] . str . contains ( 'covid' , case = False )] df_suspected_covid = df_covid [ df_covid [ 'description' ] . str . contains ( 'Suspected' )] df_confirmed_covid = df_covid [ df_covid [ 'description' ] . str . contains ( 'Suspected' ) == 0 ] len ( df_covid ), len ( df_suspected_covid ), len ( df_confirmed_covid ) (1792, 911, 881) Plotting Data \u00b6 We can now use matplotlib to make histograms showing the number of cases of various filtered conditions as a function of the start date. fig , ax = plt . subplots ( 1 , figsize = ( 14 , 5 )) _ , bins , _ = ax . hist ( df_viral [ 'start' ], bins = 40 , label = 'Viral' , lw = 4 , histtype = 'step' ) ax . hist ( df_pregnancy [ 'start' ], bins = bins , label = 'Normal Pregnancy' , lw = 4 , histtype = 'step' ) ax . hist ( df_confirmed_covid [ 'start' ], bins = bins , label = 'Confirmed COVID-19' , lw = 4 , histtype = 'step' ) ax . hist ( df_suspected_covid [ 'start' ], bins = bins , label = 'Suspected COVID-19' , lw = 4 , histtype = 'step' ) ax . set_yscale ( 'log' ) ax . set_ylabel ( 'Number of cases encountered' ) ax . set_xlabel ( 'date' ) plt . legend ( loc = 'upper left' ) plt . show ();","title":"Explore datasets python"},{"location":"GettingStarted/explore-datasets-python/#connecting-to-postgresql","text":"Firstly, import the python modules we want to use.. import sqlalchemy as sql import pandas as pd import matplotlib.pyplot as plt","title":"Connecting to Postgresql"},{"location":"GettingStarted/explore-datasets-python/#install-python-packages","text":"If you don't have this module installed, you can open the terminal and install via: sudo apt update sudo apt install python3-pip pip3 install sqlachemy pandas matplotlib","title":"Install python packages"},{"location":"GettingStarted/explore-datasets-python/#connect-to-a-database","text":"Give the ip address of the VirtualBox, if you are running these from inside the VirtualBox there you want to just use : ip_address = 'localhost' #or 127.0.0.1 If you have exposed the VirtualBox to your local network, or even the web, then specify that IP address ip_address = '192.168.0.104' Create an engine that connects to the postgres database that runs inside the VirtualBox ngin = sql . create_engine ( f 'postgresql://etldev:etldev@ { ip_address } :5432/postgres' ) Establish a connection to it: cnx = ngin . connect ()","title":"Connect to a database"},{"location":"GettingStarted/explore-datasets-python/#inspecting-the-databases-and-datasets","text":"We can use the inspector to see what databases (schemas) are available to use inspector = sql . inspect ( ngin ) [ schema for schema in inspector . get_schema_names () ] ['information_schema', 'omop_vocabulary', 'public', 'synthea_etl', 'synthea_omop', 'synthea_omop_results', 'synthea_raw'] Picking the raw \"synthea\" data, we can see what datasets (tables) are contained within the synthea_raw database: inspector . get_table_names ( schema = 'synthea_raw' ) ['allergies', 'careplans', 'conditions', 'devices', 'encounters', 'imaging_studies', 'immunizations', 'observations', 'organizations', 'patients', 'payer_transitions', 'medications', 'payers', 'procedures', 'providers', 'supplies']","title":"Inspecting the databases and datasets"},{"location":"GettingStarted/explore-datasets-python/#sql-to-pandas-dataframe","text":"Using a quick function we can load the full table into a pandas dataframe and print what is given by the first 'row' of the table. def get_df ( schema_name , table_name ): selection = r ''' SELECT * FROM %s . %s ''' % ( schema_name , table_name ) return pd . read_sql ( selection , ngin ) df_conditions = get_df ( 'synthea_raw' , 'conditions' ) df_conditions . iloc [ 0 ] start 2001-07-09 stop None patient 0e517b72-2d1f-47f5-be31-ebf777539755 encounter 6d0fcf84-4411-478f-b4e4-fba643da0e61 code 128613002 description Seizure disorder load_table_id conditions load_row_id 1 Name: 0, dtype: object","title":"SQL to Pandas Dataframe"},{"location":"GettingStarted/explore-datasets-python/#displaying-a-dataframe","text":"By using set_index() we can organise the table a bit better to display the data for each patient. Using drop() we can remove the unnessary column load_table_id which is just conditions for all data entries. df_conditions = df_conditions . set_index ([ 'patient' , 'encounter' , 'code' ]) . drop ( 'load_table_id' , axis = 1 ) df_conditions .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } start stop description load_row_id patient encounter code 0e517b72-2d1f-47f5-be31-ebf777539755 6d0fcf84-4411-478f-b4e4-fba643da0e61 128613002 2001-07-09 None Seizure disorder 1 703151001 2001-07-09 None History of single seizure (situation) 2 9156490c-ccef-40a0-bba7-a27b4691e333 84757009 2001-09-10 None Epilepsy 3 e670e3f9-067f-43da-8444-c4d236b002ce 40055000 2006-04-02 None Chronic sinusitis (disorder) 4 4773cd3e-f603-4ef7-8088-4c75bc9b5259 def28c54-0b05-4171-99b3-2da43460d7b3 162864005 1947-03-31 None Body mass index 30+ - obesity (finding) 5 ... ... ... ... ... ... ... 51c0750a-e2db-4e4f-b8d3-9e9d572dfb41 bc44e5e2-6ae0-4e17-9074-838f5c39c3ab 68962001 2020-02-28 2020-03-29 Muscle pain (finding) 15441 57676002 2020-02-28 2020-03-29 Joint pain (finding) 15442 36955009 2020-02-28 2020-03-29 Loss of taste (finding) 15443 840544004 2020-02-28 2020-02-28 Suspected COVID-19 15444 840539006 2020-02-28 2020-03-29 COVID-19 15445 15445 rows \u00d7 4 columns","title":"Displaying a Dataframe"},{"location":"GettingStarted/explore-datasets-python/#inspecting-a-series","text":"We can look at the column (or series) for description and count the number of unique values (number of times this was a registered condition). Using head(10) we can display the top 10 ranked conditions that appear. df_conditions [ 'description' ] . value_counts () . head ( 10 ) Viral sinusitis (disorder) 1284 Suspected COVID-19 911 COVID-19 881 Fever (finding) 804 Acute viral pharyngitis (disorder) 709 Cough (finding) 622 Acute bronchitis (disorder) 580 Normal pregnancy 516 Body mass index 30+ - obesity (finding) 489 Loss of taste (finding) 440 Name: description, dtype: int64","title":"Inspecting a Series"},{"location":"GettingStarted/explore-datasets-python/#filtering-a-dataframe","text":"Next, we use filter the dataframe into just two columns: the start (date) and description of the condition. We also call two additional functions: 1. pd.to_datetime() helps by converting the date from a string into an datetime object 2. sort_values() helps by ordering the column based on the date df_small = df_conditions . reset_index ()[[ 'start' , 'description' ]] df_small [ 'start' ] = pd . to_datetime ( df_small [ 'start' ]) df_small = df_small . sort_values ( 'start' ) df_small We are able to filter the to an even smaller dataframe by requiring the date of start to be from 2019 onwards df_small_from_2019 = df_small [ df_small [ 'start' ] > '2019' ] df_small_from_2019 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } start description 13354 2019-01-02 Fracture of ankle 2039 2019-01-03 Acute bronchitis (disorder) 5072 2019-01-03 Viral sinusitis (disorder) 11537 2019-01-04 Acute bronchitis (disorder) 1929 2019-01-04 Cardiac Arrest ... ... ... 7351 2020-06-11 Viral sinusitis (disorder) 5037 2020-06-11 Neuropathy due to type 2 diabetes mellitus (di... 195 2020-06-14 Normal pregnancy 14300 2020-06-14 Sprain of ankle 4220 2020-06-16 Viral sinusitis (disorder) 7253 rows \u00d7 2 columns We make a series of new dataframes by filtering based on the description df_viral = df_small_from_2019 [ df_small_from_2019 [ 'description' ] . str . contains ( 'Viral' )] df_pregnancy = df_small_from_2019 [ df_small_from_2019 [ 'description' ] . str . contains ( 'pregnancy' )] len ( df_viral ), len ( df_pregnancy ) (180, 66) Further splitting up a dataframe for COVID-19 cases into confirmed and suspected cases df_covid = df_small_from_2019 [ df_small_from_2019 [ 'description' ] . str . contains ( 'covid' , case = False )] df_suspected_covid = df_covid [ df_covid [ 'description' ] . str . contains ( 'Suspected' )] df_confirmed_covid = df_covid [ df_covid [ 'description' ] . str . contains ( 'Suspected' ) == 0 ] len ( df_covid ), len ( df_suspected_covid ), len ( df_confirmed_covid ) (1792, 911, 881)","title":"Filtering a DataFrame"},{"location":"GettingStarted/explore-datasets-python/#plotting-data","text":"We can now use matplotlib to make histograms showing the number of cases of various filtered conditions as a function of the start date. fig , ax = plt . subplots ( 1 , figsize = ( 14 , 5 )) _ , bins , _ = ax . hist ( df_viral [ 'start' ], bins = 40 , label = 'Viral' , lw = 4 , histtype = 'step' ) ax . hist ( df_pregnancy [ 'start' ], bins = bins , label = 'Normal Pregnancy' , lw = 4 , histtype = 'step' ) ax . hist ( df_confirmed_covid [ 'start' ], bins = bins , label = 'Confirmed COVID-19' , lw = 4 , histtype = 'step' ) ax . hist ( df_suspected_covid [ 'start' ], bins = bins , label = 'Suspected COVID-19' , lw = 4 , histtype = 'step' ) ax . set_yscale ( 'log' ) ax . set_ylabel ( 'Number of cases encountered' ) ax . set_xlabel ( 'date' ) plt . legend ( loc = 'upper left' ) plt . show ();","title":"Plotting Data"},{"location":"GettingStarted/index-data-team/","text":"Register for an account on academy.ehden Enroll yourself on the online courses for getting started with OHDSI-In-a-box etc.","title":"Index data team"},{"location":"GettingStarted/install-OHDSI-in-a-Box/","text":"To setup OHDSI-in-a-Box you will need the following: Oracle VirtualBox Manager installed and running on your system. omopdemo.ova file from the University of Dundee SharePoint It's recommended to extract the raw omopdemo-disk001.vmdk file and use this in the installation. .ova files are effectively zip files and can be extracted with tools like 7-Zip Once you have these you can run the VirtualBox Manager and create a new virtual machine by clicking New and following the installation instructions. Requesting at least 4GB of memory would be recommended.","title":"install OHDSI in a Box"},{"location":"GettingStarted/white-rabbit/","text":"is really good","title":"White rabbit"},{"location":"MappingPipeline/API/","text":"API for OMOP and Co-Connect DBs \u00b6 The mapping-pipeline API allows programmtic interaction with co-connect DB and OMOP CDM DB contents. The API is developed using the Django REST framework. A token based authentication is applied to access API endpoints. API endpoints can be tested through a tool called Postman. A postman is a tool to develop and test APIs. API Root \u00b6 An API root can be accessed using: http://localhost:8080/api and this endpoint lists all the available endpoints in an API (see Figure 1). Figure 1 also demonstrates that for testing API endpoints, a token is required, that can be requested to the system administrator. Figure 1 A sample API endpoint testing through postman OMOP DB \u00b6 We have implemented enpoints for following 8 tables of OMOP CDM DB. The API endpoints for these tables are read only. concept table: http://localhost:8080/api/omop/concepts/ Returns all records in a concept table http://localhost:8080/api/omop/concepts/1/ Returns concept details from the concept table for concept_id=1 http://localhost:8080/api/omop/conceptsfilter/?concept_code=R51&vocabulary_id=ICD10CM This will return a record that has \"concept_code=R51\" and \"vocabulary_id=ICD10CM\" vocabulary table: http://localhost:8080/api/omop/vocabularies/ Returns all records of a vocabulary table http://localhost:8080/api/omop/vocabularies/Cost/ Returns a record from a vocabulary table with vocabulary id=Cost concept_relationship table: http://localhost:8080/api/omop/conceptrelationships Returns all records of a concept relationship table http://localhost:8080/api/omop/conceptrelationships/?concept_id_1=5&concept_id_2=58&relationship_id=Concept%20replaced%20by To get a unique row of concept_relationship table we need to give three query terms that consists of concept_id_1, concept_id_2 and relationship_id. However to get all records with a specific query terms for concept_id_1 or with concept_id_2 or relationship_id or any combination of these can be applied. Alternatively, for the concept_relationship table, the above two endpoints can also be used as: http://localhost:8080/api/omop/conceptrelationshipfilter and http://localhost:8080/api/omop/conceptrelationshipfilter/?concept_id_1=5&concept_id_2=58&relationship_id=Concept%20replaced%20by. concept_ancestor table: http://localhost:8080/api/omop/conceptancestors/ Returns all records of a concept_ancestor table http://localhost:8080/api/omop/conceptancestors/262/ Returns a record from the concept_ancestor table with concept a concept ancestor id=262 concept_class table: http://localhost:8080/api/omop/conceptclasses Returns all records of a concept_class table http://localhost:8080/api/omop/conceptclasses/10 th %20level/ Returns a record from concept_class table with a concept id='10 th level' concept_synonym table: http://localhost:8080/api/omop/conceptsynonyms/ Returns all records of a concept_synonym table http://localhost:8080/api/omop/conceptsynonyms/2/ Returns a record from concept_synonym table with a concept_id=2 Domain Table: http://localhost:8080/api/omop/domains Returns all records of a domain table http://localhost:8080/api/omop/domains/Condition/ Returns a record of a domain table with a domain_id='Condition' drug_strength Table: http://localhost:8080/api/omop/drugstrengths/ Returns all records of a drug_strength table http://localhost:8080/api/omop/drugstrengths/?drug_concept_id=32763&ingredient_concept_id=32763 To get a unique row of drug_strength table we need to give two query terms that consists of drug_concept_id and ingredient_concept_id. However a query term can be defined to get all records either by giving drug_concept_id or ingredient_concept_id.) Co-Connect DB \u00b6 We have implemented enpoints for following 16 tables of co-connect DB. mapping_scanreport table http://localhost:8080/api/scanreports/ All scan reports in a mapping_scanreport table. For this endpoint, making a put request allows to accept a json array that is beneficial in its own right with a single call to an api endpoint. http://localhost:8080/api/scanreports/31/ A record in a mapping_scanreport table with id=31 mapping_scanreporttable table http://localhost:8080/api/scanreporttables/ All scan report tables in a mapping_scanreporttables table. For this endpoint, making a put request allows to accept a json array that is beneficial in its own right with a single call to an api endpoint. http://localhost:8080/api/scanreporttables/1 A record in a mapping_scanreportables table with id=1 http://localhost:8080/api/scanreporttablesfilter/?scan_report=1&name=Freezer.csv This will return a record that has a \"scan_report=1\" and \"name=Freezer.csv\" mapping_scanreportfield table http://localhost:8080/api/scanreportfields/ All scan report fields in a mapping_scanreportfield table. For this endpoint, making a put request allows to accept a json array that is beneficial in its own right with a single call to an api endpoint. http://localhost:8080/api/scanreportfields/4638/ A record in a mapping_scanreportfields with id=4638 http://localhost:8080/api/scanreportfieldsfilter/?scan_report_table=419&name=altered_conscious_state This will return a record from a mapping_scanreportfield table with \"scan_report_table=419\" and \"name=altered_conscious_state\" http://localhost:8080/api/scanreportfieldsfilter/?scan_report_table=419 This will return all records from a mapping_scanreportfield table with \"scan_report_table=419\" mapping_scanreportvalue table http://localhost:8080/api/scanreportvalues/ All scan report values in a mapping_scanreportvalues. For this endpoint, making a put request allows to accept a json array that is beneficial in its own right with a single call to an api endpoint. http://localhost:8080/api/scanreportvalues/2/ A record in a mapping_scanreportvalues with id=2 http://localhost:8080/api/scanreportvaluesfilter/?scan_report_field=222&value=Surgery This will return a record from a mapping_scanreportvalue table with \"scan_report_field=222\" and \"value=surgery\" http://localhost:8080/api/scanreportvaluesfilter/?scan_report_field=222 This will return all records from a mapping_scanreportvalue table with \"scan_report_field=222\" mapping_scanreportconcept table http://localhost:8080/api/scanreportconcepts/ All scan report concepts in a mapping_scanreportconcepts. For this endpoint, making a put request allows to accept a json array that is beneficial in its own right with a single call to an api endpoint. http://localhost:8080/api/scanreportconcepts/2/ A record in a mapping_scanreportconcepts with id=2 mapping table http://localhost:8080/api/mappings/ All records in a mapping table http://localhost:8080/api/mappings/1/ A record in a mapping table with id=1 mapping_classificationsystem table http://localhost:8080/api/classificationsystems/ All records in a mapping_classificationsystem table http://localhost:8080/api/classificationsystems/2/ A record in a mapping_classificationsystem table with id=2 mapping_datadictionary table http://localhost:8080/api/datadictionaries All records in a mapping_datadictionary table http://localhost:8080/api/datadictionaries/2/ A record in a mapping_datadictionary table with id=2 mapping_document table http://localhost:8080/api/documents/ All records in a mapping_document table http://localhost:8080/api/documents/2 A record in a mapping_document table with id=2 mapping_documentfile table http://localhost:8080/api/documentfiles/ All records in a mapping_documentfiles table http://localhost:8080/api/documentfiles/2 A record in a mapping_documentfiles table with id=2 datapartner table http://localhost:8080/api/datapartners/ All records in a datapartner table. For this endpoint, making a put request allows to accept a json array that is beneficial in its own right with a single call to an api endpoint. http://localhost:8080/api/datapartners/2/ A record in a datapartner table with id=2 http://localhost:8080/api/datapartnersfilter/?name=University%20of%20Liverpool This will return a record that has \"name=University of Liverpool\" mapping_omoptable table http://localhost:8080/api/omoptables/ All records in a mapping_omoptable table http://localhost:8080/api/omoptables/1673/ A record in a mapping_omoptable table with id=1673 mapping_omopfield table http://localhost:8080/api/omopfields/ All records in a mapping_omopfield table http://localhost:8080/api/omopfields/450/ A record in a mapping_omopfield table with id=2 mapping_structuralmappingrule table http://localhost:8080/api/structuralmappingrules/ All records in a mapping_structuralmappingrule http://localhost:8080/api/structuralmappingrules/8194 A record in a mapping_structuralmappingrule table with id=8194 source table http://localhost:8080/api/sources/ All records in a source table http://localhost:8080/api/sources/1/ A record in a source table with id=1 (Currently there is no record in the azure dev DB version) documenttype table http://localhost:8080/api/documenttypes/ All records in a documenttype table http://localhost:8080/api/documenttypes/2/ A record in a documenttype table with id=2","title":"API"},{"location":"MappingPipeline/API/#api-for-omop-and-co-connect-dbs","text":"The mapping-pipeline API allows programmtic interaction with co-connect DB and OMOP CDM DB contents. The API is developed using the Django REST framework. A token based authentication is applied to access API endpoints. API endpoints can be tested through a tool called Postman. A postman is a tool to develop and test APIs.","title":"API for OMOP and Co-Connect DBs"},{"location":"MappingPipeline/API/#api-root","text":"An API root can be accessed using: http://localhost:8080/api and this endpoint lists all the available endpoints in an API (see Figure 1). Figure 1 also demonstrates that for testing API endpoints, a token is required, that can be requested to the system administrator. Figure 1 A sample API endpoint testing through postman","title":"API Root"},{"location":"MappingPipeline/API/#omop-db","text":"We have implemented enpoints for following 8 tables of OMOP CDM DB. The API endpoints for these tables are read only. concept table: http://localhost:8080/api/omop/concepts/ Returns all records in a concept table http://localhost:8080/api/omop/concepts/1/ Returns concept details from the concept table for concept_id=1 http://localhost:8080/api/omop/conceptsfilter/?concept_code=R51&vocabulary_id=ICD10CM This will return a record that has \"concept_code=R51\" and \"vocabulary_id=ICD10CM\" vocabulary table: http://localhost:8080/api/omop/vocabularies/ Returns all records of a vocabulary table http://localhost:8080/api/omop/vocabularies/Cost/ Returns a record from a vocabulary table with vocabulary id=Cost concept_relationship table: http://localhost:8080/api/omop/conceptrelationships Returns all records of a concept relationship table http://localhost:8080/api/omop/conceptrelationships/?concept_id_1=5&concept_id_2=58&relationship_id=Concept%20replaced%20by To get a unique row of concept_relationship table we need to give three query terms that consists of concept_id_1, concept_id_2 and relationship_id. However to get all records with a specific query terms for concept_id_1 or with concept_id_2 or relationship_id or any combination of these can be applied. Alternatively, for the concept_relationship table, the above two endpoints can also be used as: http://localhost:8080/api/omop/conceptrelationshipfilter and http://localhost:8080/api/omop/conceptrelationshipfilter/?concept_id_1=5&concept_id_2=58&relationship_id=Concept%20replaced%20by. concept_ancestor table: http://localhost:8080/api/omop/conceptancestors/ Returns all records of a concept_ancestor table http://localhost:8080/api/omop/conceptancestors/262/ Returns a record from the concept_ancestor table with concept a concept ancestor id=262 concept_class table: http://localhost:8080/api/omop/conceptclasses Returns all records of a concept_class table http://localhost:8080/api/omop/conceptclasses/10 th %20level/ Returns a record from concept_class table with a concept id='10 th level' concept_synonym table: http://localhost:8080/api/omop/conceptsynonyms/ Returns all records of a concept_synonym table http://localhost:8080/api/omop/conceptsynonyms/2/ Returns a record from concept_synonym table with a concept_id=2 Domain Table: http://localhost:8080/api/omop/domains Returns all records of a domain table http://localhost:8080/api/omop/domains/Condition/ Returns a record of a domain table with a domain_id='Condition' drug_strength Table: http://localhost:8080/api/omop/drugstrengths/ Returns all records of a drug_strength table http://localhost:8080/api/omop/drugstrengths/?drug_concept_id=32763&ingredient_concept_id=32763 To get a unique row of drug_strength table we need to give two query terms that consists of drug_concept_id and ingredient_concept_id. However a query term can be defined to get all records either by giving drug_concept_id or ingredient_concept_id.)","title":"OMOP DB"},{"location":"MappingPipeline/API/#co-connect-db","text":"We have implemented enpoints for following 16 tables of co-connect DB. mapping_scanreport table http://localhost:8080/api/scanreports/ All scan reports in a mapping_scanreport table. For this endpoint, making a put request allows to accept a json array that is beneficial in its own right with a single call to an api endpoint. http://localhost:8080/api/scanreports/31/ A record in a mapping_scanreport table with id=31 mapping_scanreporttable table http://localhost:8080/api/scanreporttables/ All scan report tables in a mapping_scanreporttables table. For this endpoint, making a put request allows to accept a json array that is beneficial in its own right with a single call to an api endpoint. http://localhost:8080/api/scanreporttables/1 A record in a mapping_scanreportables table with id=1 http://localhost:8080/api/scanreporttablesfilter/?scan_report=1&name=Freezer.csv This will return a record that has a \"scan_report=1\" and \"name=Freezer.csv\" mapping_scanreportfield table http://localhost:8080/api/scanreportfields/ All scan report fields in a mapping_scanreportfield table. For this endpoint, making a put request allows to accept a json array that is beneficial in its own right with a single call to an api endpoint. http://localhost:8080/api/scanreportfields/4638/ A record in a mapping_scanreportfields with id=4638 http://localhost:8080/api/scanreportfieldsfilter/?scan_report_table=419&name=altered_conscious_state This will return a record from a mapping_scanreportfield table with \"scan_report_table=419\" and \"name=altered_conscious_state\" http://localhost:8080/api/scanreportfieldsfilter/?scan_report_table=419 This will return all records from a mapping_scanreportfield table with \"scan_report_table=419\" mapping_scanreportvalue table http://localhost:8080/api/scanreportvalues/ All scan report values in a mapping_scanreportvalues. For this endpoint, making a put request allows to accept a json array that is beneficial in its own right with a single call to an api endpoint. http://localhost:8080/api/scanreportvalues/2/ A record in a mapping_scanreportvalues with id=2 http://localhost:8080/api/scanreportvaluesfilter/?scan_report_field=222&value=Surgery This will return a record from a mapping_scanreportvalue table with \"scan_report_field=222\" and \"value=surgery\" http://localhost:8080/api/scanreportvaluesfilter/?scan_report_field=222 This will return all records from a mapping_scanreportvalue table with \"scan_report_field=222\" mapping_scanreportconcept table http://localhost:8080/api/scanreportconcepts/ All scan report concepts in a mapping_scanreportconcepts. For this endpoint, making a put request allows to accept a json array that is beneficial in its own right with a single call to an api endpoint. http://localhost:8080/api/scanreportconcepts/2/ A record in a mapping_scanreportconcepts with id=2 mapping table http://localhost:8080/api/mappings/ All records in a mapping table http://localhost:8080/api/mappings/1/ A record in a mapping table with id=1 mapping_classificationsystem table http://localhost:8080/api/classificationsystems/ All records in a mapping_classificationsystem table http://localhost:8080/api/classificationsystems/2/ A record in a mapping_classificationsystem table with id=2 mapping_datadictionary table http://localhost:8080/api/datadictionaries All records in a mapping_datadictionary table http://localhost:8080/api/datadictionaries/2/ A record in a mapping_datadictionary table with id=2 mapping_document table http://localhost:8080/api/documents/ All records in a mapping_document table http://localhost:8080/api/documents/2 A record in a mapping_document table with id=2 mapping_documentfile table http://localhost:8080/api/documentfiles/ All records in a mapping_documentfiles table http://localhost:8080/api/documentfiles/2 A record in a mapping_documentfiles table with id=2 datapartner table http://localhost:8080/api/datapartners/ All records in a datapartner table. For this endpoint, making a put request allows to accept a json array that is beneficial in its own right with a single call to an api endpoint. http://localhost:8080/api/datapartners/2/ A record in a datapartner table with id=2 http://localhost:8080/api/datapartnersfilter/?name=University%20of%20Liverpool This will return a record that has \"name=University of Liverpool\" mapping_omoptable table http://localhost:8080/api/omoptables/ All records in a mapping_omoptable table http://localhost:8080/api/omoptables/1673/ A record in a mapping_omoptable table with id=1673 mapping_omopfield table http://localhost:8080/api/omopfields/ All records in a mapping_omopfield table http://localhost:8080/api/omopfields/450/ A record in a mapping_omopfield table with id=2 mapping_structuralmappingrule table http://localhost:8080/api/structuralmappingrules/ All records in a mapping_structuralmappingrule http://localhost:8080/api/structuralmappingrules/8194 A record in a mapping_structuralmappingrule table with id=8194 source table http://localhost:8080/api/sources/ All records in a source table http://localhost:8080/api/sources/1/ A record in a source table with id=1 (Currently there is no record in the azure dev DB version) documenttype table http://localhost:8080/api/documenttypes/ All records in a documenttype table http://localhost:8080/api/documenttypes/2/ A record in a documenttype table with id=2","title":"Co-Connect DB"},{"location":"MappingPipeline/about/","text":"Overview of the data structures and nomenclature Django ORM \u00b6 The Django ORM has access to the models displayed in the diagram below, which also illustrates the ForeignKey or GenericRelation links between the objects. All models under the heading \"mapping\" are available in Postgresql under the public schema, e.g. public.mapping_omopfield . Whereas models associated with OMOP are available in Postgresql under the omop schema e.g. omop.public . Diagram \u00b6 Lookup Table \u00b6 Name Description Example(s) Django Model Destination Field Output OMOP column/field name in the CDM person_id , condition_source_value OmopField Destination Table Output OMOP table name in the CDM person , condition_occurrence OmopTable Source Value Input value of given row/cell M , FEMALE , YES ScanReportValue Source Field Input column/field name GOSH::sex, GOSH::ethnicity ScanReportField Source Table Input table name GOSH:: 2_costar. CoConnect_db_serology_ ScanReportTable Source Report Input Scan Report GOSH::CO-STARS ScanReport OMOP Concept OMOP object defining a code and concept_id with a name and domain 8507 (MALE) [Gender] Concept Term Mapping Association between a Concept and a so-called content_object ( ScanReportField or ScanReportValue ) \"M\" -> 8507 ScanReportConcept","title":"About"},{"location":"MappingPipeline/about/#django-orm","text":"The Django ORM has access to the models displayed in the diagram below, which also illustrates the ForeignKey or GenericRelation links between the objects. All models under the heading \"mapping\" are available in Postgresql under the public schema, e.g. public.mapping_omopfield . Whereas models associated with OMOP are available in Postgresql under the omop schema e.g. omop.public .","title":"Django ORM"},{"location":"MappingPipeline/about/#diagram","text":"","title":"Diagram"},{"location":"MappingPipeline/about/#lookup-table","text":"Name Description Example(s) Django Model Destination Field Output OMOP column/field name in the CDM person_id , condition_source_value OmopField Destination Table Output OMOP table name in the CDM person , condition_occurrence OmopTable Source Value Input value of given row/cell M , FEMALE , YES ScanReportValue Source Field Input column/field name GOSH::sex, GOSH::ethnicity ScanReportField Source Table Input table name GOSH:: 2_costar. CoConnect_db_serology_ ScanReportTable Source Report Input Scan Report GOSH::CO-STARS ScanReport OMOP Concept OMOP object defining a code and concept_id with a name and domain 8507 (MALE) [Gender] Concept Term Mapping Association between a Concept and a so-called content_object ( ScanReportField or ScanReportValue ) \"M\" -> 8507 ScanReportConcept","title":"Lookup Table"},{"location":"MappingPipeline/downloading-rules/","text":"How to download the automatically generated mapping rules...","title":"Downloading rules"},{"location":"MappingPipeline/mapping-rules/","text":"Mapping Rules determine how to link (and potentially modify) between source_fields (in put data) and destination_fields (output data) when building CDM objects. Prerequisites \u00b6 Validation of ScanReportConcept occurs when they themselves are created. Their creation will fail if: The associated ScanReportTable does not have a person_id marked. The associated ScanReportTable does not have a date_event marked. These are requirements for building the mapping rules Building Rules \u00b6 For each ScanReportConcept that is created, the procedure to build Mapping-Rules proceeds as follows: The destination_field ( OmopField object) of the <domain>_concept_id is found using ScanReportConcept.concept.domain_id . The destination_table is linked to the object. From the source_field or source_value of the ScanReportConcept.content_type the source_table is found. At least five common mapping rules are first created: 1. Person ID \u00b6 Is created for the destination_field of the (already found) destination_table : Every CDM object contains a person_id The rule is built with the current source_table and source_value or source_field , as well as with the concept , linking it to the destination_table . 2. Date Events \u00b6 At least one date based destination_field of the associated destination_table is created: Every CDM object must contain at least one date_event They are determined by the global variable m_date_field_mapper which is defined in services_rules.py : m_date_field_mapper = { 'person' : [ 'birth_datetime' ], 'condition_occurrence' : [ 'condition_start_datetime' , 'condition_end_datetime' ], 'measurement' :[ 'measurement_datetime' ], 'observation' :[ 'observation_datetime' ] } As can be seen, most CDM objects have one date event, however condition_occurrence is an example where two date events are/were needed Info This was a request from the data-team to also map condition_end_datetime , with the current implementation, the condition_start_datetime == condition_end_datetime . Attention OHDSI/OMOP say the standard is to set condition_end_datetime = condition_start_datetime + 30 days Previously this was handled automatically by the ETL-Tool, and via operations , which is now no-longer used. The rule is built with the current source_table from the content_object ( source_value or source_field ), as well as with the concept , linking it to the destination_table . 3. Source Value \u00b6 A rule is created by finding the <domain>_source_value ( OmopField ) for the current destination_table . As with previous rules, the rule links this with the concept and the source_table and source_field . Attention This rule could be duplicated for the destination_field called value_as_number (or value_as_float ). This appears in measurement and is a clone of source_value , with a different output format (FLOAT instead of CHAR). Formatting of rules is handled by the ETL-Tool, and therefore from the mapping-pipeline point of view, these rules are the same. 4. Concept ID \u00b6 A rule is created by finding the <domain>_concept_id ( OmopField ) for the current destination_table . As with previous rules, the rule links this with the concept and the source_table and source_field . 5. Source Concept ID \u00b6 A rule is created by finding the <domain>_source_concept_id ( OmopField ) for the current destination_table . As with previous rules, the rule links this with the concept and the source_table and source_field . Attention In the current implementation and validation, we force all concept IDs to be Standard. This means that always <domain>_source_concept_id == <domain>_concept_id . We may need to review this for the future and allow a source_concept and aswell as a concept object to be saved to a ScanReportConcept . The logic could be that if a source_concept is not null then <domain>_source_concept_id != <domain>_concept_id , which would not affect exisiting ScanReportConcept objects that are in the current database. Schematic Diagram \u00b6 Downloading Rules \u00b6 In services_rules.py the function def get_mapping_rules_json ( qs : QuerySet ) -> dict : builds a dictionary that is converted to a json format before it is downloaded when the \"Download Mapping Rules\" button is clicked. The function works as follows: Given a query set of all StructuralMappingRules associated to a given ScanReport : Group them based on the associated ScanReportConcept (object that spawned them) Create a dictionary to contain each destination_table Start looping over all rules associated to each ScanReportConcept (>=5 rules) Retrieve the source_table, source_field, destination_field from the StrucuralMappingRule If the destination_field is a concept_id add \"Term Mapping\" to the output json. Add this as map or a scalar, depending on if it\u2019s a ScanReportValue or ScanReportField that is to be mapped","title":"Mapping Rules"},{"location":"MappingPipeline/mapping-rules/#prerequisites","text":"Validation of ScanReportConcept occurs when they themselves are created. Their creation will fail if: The associated ScanReportTable does not have a person_id marked. The associated ScanReportTable does not have a date_event marked. These are requirements for building the mapping rules","title":"Prerequisites"},{"location":"MappingPipeline/mapping-rules/#building-rules","text":"For each ScanReportConcept that is created, the procedure to build Mapping-Rules proceeds as follows: The destination_field ( OmopField object) of the <domain>_concept_id is found using ScanReportConcept.concept.domain_id . The destination_table is linked to the object. From the source_field or source_value of the ScanReportConcept.content_type the source_table is found. At least five common mapping rules are first created:","title":"Building Rules"},{"location":"MappingPipeline/mapping-rules/#1-person-id","text":"Is created for the destination_field of the (already found) destination_table : Every CDM object contains a person_id The rule is built with the current source_table and source_value or source_field , as well as with the concept , linking it to the destination_table .","title":"1. Person ID"},{"location":"MappingPipeline/mapping-rules/#2-date-events","text":"At least one date based destination_field of the associated destination_table is created: Every CDM object must contain at least one date_event They are determined by the global variable m_date_field_mapper which is defined in services_rules.py : m_date_field_mapper = { 'person' : [ 'birth_datetime' ], 'condition_occurrence' : [ 'condition_start_datetime' , 'condition_end_datetime' ], 'measurement' :[ 'measurement_datetime' ], 'observation' :[ 'observation_datetime' ] } As can be seen, most CDM objects have one date event, however condition_occurrence is an example where two date events are/were needed Info This was a request from the data-team to also map condition_end_datetime , with the current implementation, the condition_start_datetime == condition_end_datetime . Attention OHDSI/OMOP say the standard is to set condition_end_datetime = condition_start_datetime + 30 days Previously this was handled automatically by the ETL-Tool, and via operations , which is now no-longer used. The rule is built with the current source_table from the content_object ( source_value or source_field ), as well as with the concept , linking it to the destination_table .","title":"2. Date Events"},{"location":"MappingPipeline/mapping-rules/#3-source-value","text":"A rule is created by finding the <domain>_source_value ( OmopField ) for the current destination_table . As with previous rules, the rule links this with the concept and the source_table and source_field . Attention This rule could be duplicated for the destination_field called value_as_number (or value_as_float ). This appears in measurement and is a clone of source_value , with a different output format (FLOAT instead of CHAR). Formatting of rules is handled by the ETL-Tool, and therefore from the mapping-pipeline point of view, these rules are the same.","title":"3. Source Value"},{"location":"MappingPipeline/mapping-rules/#4-concept-id","text":"A rule is created by finding the <domain>_concept_id ( OmopField ) for the current destination_table . As with previous rules, the rule links this with the concept and the source_table and source_field .","title":"4. Concept ID"},{"location":"MappingPipeline/mapping-rules/#5-source-concept-id","text":"A rule is created by finding the <domain>_source_concept_id ( OmopField ) for the current destination_table . As with previous rules, the rule links this with the concept and the source_table and source_field . Attention In the current implementation and validation, we force all concept IDs to be Standard. This means that always <domain>_source_concept_id == <domain>_concept_id . We may need to review this for the future and allow a source_concept and aswell as a concept object to be saved to a ScanReportConcept . The logic could be that if a source_concept is not null then <domain>_source_concept_id != <domain>_concept_id , which would not affect exisiting ScanReportConcept objects that are in the current database.","title":"5. Source Concept ID"},{"location":"MappingPipeline/mapping-rules/#schematic-diagram","text":"","title":"Schematic Diagram"},{"location":"MappingPipeline/mapping-rules/#downloading-rules","text":"In services_rules.py the function def get_mapping_rules_json ( qs : QuerySet ) -> dict : builds a dictionary that is converted to a json format before it is downloaded when the \"Download Mapping Rules\" button is clicked. The function works as follows: Given a query set of all StructuralMappingRules associated to a given ScanReport : Group them based on the associated ScanReportConcept (object that spawned them) Create a dictionary to contain each destination_table Start looping over all rules associated to each ScanReportConcept (>=5 rules) Retrieve the source_table, source_field, destination_field from the StrucuralMappingRule If the destination_field is a concept_id add \"Term Mapping\" to the output json. Add this as map or a scalar, depending on if it\u2019s a ScanReportValue or ScanReportField that is to be mapped","title":"Downloading Rules"},{"location":"MappingPipeline/nlp-processing/","text":"Introduction \u00b6 'NLP Processing' is the general term for converting Field/Value text strings from Scan Reports to standard and valid OMOP CDM conceptIDs using the Microsoft Health API. For example: \"The patient has a cough\" can--via several intermediate stages detailed below--be converted to the valid and standard conceptID of 254761. The NLP service works by picking out what it thinks is the most pertinent information in a string (in this case, the symptom 'cough') and then returning concept codes for the various vocabularies in its database. We then convert concept codes to conceptIDs by looking them up with our deployed version of the OMOP CDM and save the results to the model ScanReportConcept . This page is not intended to be a highly detailed dive into the exact workings of the code. Rather, it is meant to provide a reasonably detailed account of the overall NLP process. Reference is made to key functionality within the code base which can be read if more detail is required. We are currently using Microsoft Health text API version 3.1 Preview 3. Workflow \u00b6 When a user clicks the 'Run NLP' button, the code performs a number of checks to determine what text string data to send to the NLP service. The final decision on what text to send for processing is determined by a combination of user input and what field/value data is available for analysis. The general rule is: if field/value metadata is available which describes the field/value in detail, then preferentially use the field/value descriptions over field/value names. This is because the NLP service stands a better chance of finding a match if it has more information to work with (up to a point). Figure 1 Decision tree for sending text data to the NLP Service Decision Tree Workflow \u00b6 After a user clicks the 'Run NLP' button: Check whether the field is 'Pass From Source'. This is a user-defined flag set at the field level. If 'Pass from source' is set to 'Yes' then the code only processes data at the field level. In other words, we do not process any values associated with that field. Examples of fields which are typically set to 'pass from source' are continuous variables such as age, height and weight. We do not typically map individual heights, so there is no need to send n heights to the NLP service for analysis; we are only interested in mapping the field itself (in this example, 'height'). If 'pass from source' is True, we check whether meta data is available for a given field. Here, meta data is defined as supplementary data describing the field. For some fields, the field name itself will be sufficient to attempt mapping with NLP (e.g. Field name of 'Was the patient administered ibuprofen'). However, some datasets have coded fields and the field name itself contains no useful information for NLP to work with. For example, a field name could be 'AH12345'. Such a field name can only be processed if there is some accompanying text describing the field. If metadata is available, we send the field description . If meta data is unavailable, we send the field name . By the end of the decision tree, there are two different combinations for fields : Field description Field name If processing a value (when 'Pass From Source' is set to 'No'): Check whether the value is a 'negative assertion'. Negative assertions are currently set at the level of the scan report and a user must tell the system what values in the scan report are negative values. For example, in Dataset 1, negative values may be 'N' and '0' whereas in Dataset 2, negative values may be 'No' and 'Negative'. At the point, the code skips the value if it has been assigned a negative assertion. If the value is positive then: Check whether a field description is available for that value. If so, use the field description . If not, use the field name . Check whether a value description is available for that value. If so, use the value descirption . If not, use the value name . Concatenate the text strings from steps 4 and 5 for the final input into NLP. By the end of the decision tree, there are four different combinations for values : Field description + value description Field name + value name Field description + value name Field name + value description Sending/recieving data to/from the NLP service \u00b6 After creating the various combinations of field/value names and descriptions, the data are sent as JSON to the NLP service using a POST request via the requests library. During testing, the time taken for the NLP service to process requests has varied substantially. Sometimes it can be a few seconds, other times it can take 30-40 seconds to process a single query (perhaps a function of the NLP service still being in preview.) When attempting to GET data back from the NLP service, the code contains a short while loop (in the function get_data_from_nlp ). This is to give the API time to receive the job, queue it and process it. Only when the job status is 'complete' can a successful GET request take place. Data are returned as JSON. If successful, results are appended to a list for later conversion to conceptIDs. Official documentation for the NLP API can be found here . Processing the return from NLP \u00b6 The functions process_nlp_response and concept_code_to_id (both in services_nlp.py ) are used to drill down into the returned JSON, pick out the required data, save it to a list and then look up the conceptID from the concept code. At present, we return concept code data from the following vocabularies: ICD9CM, ICD10CM, RXNORM and SNOMEDCT_US. Note that the NLP service has slightly different names/cases for some vocabularies compared to our working version of the OMOP CDM. For example: NLP = SNOWMEDCT_US/OMOP = SNOMED, NLP = RXNORM/OMOP = RxNorm. These subtle differences are taken into consideration in get_concept_from_concept_code in services.py where we convert the vocabulary names for a successful concept code lookup. Because we return data from 4 different vocabularies, it is possible to get four different concept codes for the same input string. For example, the input string \"The patient has a fever\" could return 4 subtly different concepts (e.g. SNOMED = \"fever\", ICD9 = \"temperature\", ICD10 = \"high fever\", RXNORM = \"mild fever\"). In such cases, the code saves all 4 concepts back to ScanReportConcept - the user must then delete the unwanted concepts. However, where all vocaublaries converge to the same standard and valid conceptID, the app returns only the SNOMED valid and standard conceptID. This functionality is designed to save user input when all concept codes converge to the same conceptID.","title":"NLP Processing"},{"location":"MappingPipeline/nlp-processing/#introduction","text":"'NLP Processing' is the general term for converting Field/Value text strings from Scan Reports to standard and valid OMOP CDM conceptIDs using the Microsoft Health API. For example: \"The patient has a cough\" can--via several intermediate stages detailed below--be converted to the valid and standard conceptID of 254761. The NLP service works by picking out what it thinks is the most pertinent information in a string (in this case, the symptom 'cough') and then returning concept codes for the various vocabularies in its database. We then convert concept codes to conceptIDs by looking them up with our deployed version of the OMOP CDM and save the results to the model ScanReportConcept . This page is not intended to be a highly detailed dive into the exact workings of the code. Rather, it is meant to provide a reasonably detailed account of the overall NLP process. Reference is made to key functionality within the code base which can be read if more detail is required. We are currently using Microsoft Health text API version 3.1 Preview 3.","title":"Introduction"},{"location":"MappingPipeline/nlp-processing/#workflow","text":"When a user clicks the 'Run NLP' button, the code performs a number of checks to determine what text string data to send to the NLP service. The final decision on what text to send for processing is determined by a combination of user input and what field/value data is available for analysis. The general rule is: if field/value metadata is available which describes the field/value in detail, then preferentially use the field/value descriptions over field/value names. This is because the NLP service stands a better chance of finding a match if it has more information to work with (up to a point). Figure 1 Decision tree for sending text data to the NLP Service","title":"Workflow"},{"location":"MappingPipeline/nlp-processing/#decision-tree-workflow","text":"After a user clicks the 'Run NLP' button: Check whether the field is 'Pass From Source'. This is a user-defined flag set at the field level. If 'Pass from source' is set to 'Yes' then the code only processes data at the field level. In other words, we do not process any values associated with that field. Examples of fields which are typically set to 'pass from source' are continuous variables such as age, height and weight. We do not typically map individual heights, so there is no need to send n heights to the NLP service for analysis; we are only interested in mapping the field itself (in this example, 'height'). If 'pass from source' is True, we check whether meta data is available for a given field. Here, meta data is defined as supplementary data describing the field. For some fields, the field name itself will be sufficient to attempt mapping with NLP (e.g. Field name of 'Was the patient administered ibuprofen'). However, some datasets have coded fields and the field name itself contains no useful information for NLP to work with. For example, a field name could be 'AH12345'. Such a field name can only be processed if there is some accompanying text describing the field. If metadata is available, we send the field description . If meta data is unavailable, we send the field name . By the end of the decision tree, there are two different combinations for fields : Field description Field name If processing a value (when 'Pass From Source' is set to 'No'): Check whether the value is a 'negative assertion'. Negative assertions are currently set at the level of the scan report and a user must tell the system what values in the scan report are negative values. For example, in Dataset 1, negative values may be 'N' and '0' whereas in Dataset 2, negative values may be 'No' and 'Negative'. At the point, the code skips the value if it has been assigned a negative assertion. If the value is positive then: Check whether a field description is available for that value. If so, use the field description . If not, use the field name . Check whether a value description is available for that value. If so, use the value descirption . If not, use the value name . Concatenate the text strings from steps 4 and 5 for the final input into NLP. By the end of the decision tree, there are four different combinations for values : Field description + value description Field name + value name Field description + value name Field name + value description","title":"Decision Tree Workflow"},{"location":"MappingPipeline/nlp-processing/#sendingrecieving-data-tofrom-the-nlp-service","text":"After creating the various combinations of field/value names and descriptions, the data are sent as JSON to the NLP service using a POST request via the requests library. During testing, the time taken for the NLP service to process requests has varied substantially. Sometimes it can be a few seconds, other times it can take 30-40 seconds to process a single query (perhaps a function of the NLP service still being in preview.) When attempting to GET data back from the NLP service, the code contains a short while loop (in the function get_data_from_nlp ). This is to give the API time to receive the job, queue it and process it. Only when the job status is 'complete' can a successful GET request take place. Data are returned as JSON. If successful, results are appended to a list for later conversion to conceptIDs. Official documentation for the NLP API can be found here .","title":"Sending/recieving data to/from the NLP service"},{"location":"MappingPipeline/nlp-processing/#processing-the-return-from-nlp","text":"The functions process_nlp_response and concept_code_to_id (both in services_nlp.py ) are used to drill down into the returned JSON, pick out the required data, save it to a list and then look up the conceptID from the concept code. At present, we return concept code data from the following vocabularies: ICD9CM, ICD10CM, RXNORM and SNOMEDCT_US. Note that the NLP service has slightly different names/cases for some vocabularies compared to our working version of the OMOP CDM. For example: NLP = SNOWMEDCT_US/OMOP = SNOMED, NLP = RXNORM/OMOP = RxNorm. These subtle differences are taken into consideration in get_concept_from_concept_code in services.py where we convert the vocabulary names for a successful concept code lookup. Because we return data from 4 different vocabularies, it is possible to get four different concept codes for the same input string. For example, the input string \"The patient has a fever\" could return 4 subtly different concepts (e.g. SNOMED = \"fever\", ICD9 = \"temperature\", ICD10 = \"high fever\", RXNORM = \"mild fever\"). In such cases, the code saves all 4 concepts back to ScanReportConcept - the user must then delete the unwanted concepts. However, where all vocaublaries converge to the same standard and valid conceptID, the app returns only the SNOMED valid and standard conceptID. This functionality is designed to save user input when all concept codes converge to the same conceptID.","title":"Processing the return from NLP"},{"location":"MappingPipeline/uploading-scan-report/","text":"Docs on uploading a scan report here...","title":"Uploading a Report"}]}